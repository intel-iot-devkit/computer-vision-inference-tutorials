From 18c0bd9224ea2764698bc42ceba47f1be4ed7967 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Micha=C5=82=20Winiarski?= <michal.winiarski@intel.com>
Date: Wed, 21 Oct 2015 13:11:24 +0200
Subject: [PATCH 01/17] drm/i915: Add L3_LRA_1 Register to cmdparser whitelist
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Change-Id: I4fba318061b175d481ea2f2f02593d70930daf23
Signed-off-by: Michał Winiarski <michal.winiarski@intel.com>
---
 drivers/gpu/drm/i915/i915_cmd_parser.c |    1 +
 1 files changed, 1 insertions(+), 0 deletions(-)

diff --git a/drivers/gpu/drm/i915/i915_cmd_parser.c b/drivers/gpu/drm/i915/i915_cmd_parser.c
index a337f33..85f0f2f 100644
--- a/drivers/gpu/drm/i915/i915_cmd_parser.c
+++ b/drivers/gpu/drm/i915/i915_cmd_parser.c
@@ -457,6 +457,7 @@ static const struct drm_i915_reg_descriptor gen7_render_regs[] = {
 	REG32(GEN7_GPGPU_DISPATCHDIMX),
 	REG32(GEN7_GPGPU_DISPATCHDIMY),
 	REG32(GEN7_GPGPU_DISPATCHDIMZ),
+	REG32(GEN7_LRA_LIMITS(1)),
 	REG64_IDX(GEN7_SO_NUM_PRIMS_WRITTEN, 0),
 	REG64_IDX(GEN7_SO_NUM_PRIMS_WRITTEN, 1),
 	REG64_IDX(GEN7_SO_NUM_PRIMS_WRITTEN, 2),
-- 
1.7.1


From 65d41df980d5b2728b5da40434c4fbda28a63419 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Micha=C5=82=20Winiarski?= <michal.winiarski@intel.com>
Date: Fri, 29 Jan 2016 13:25:34 +0100
Subject: [PATCH 02/17] drm/i915: Android MOCS on top of upstream
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Extending the ABI instead of breaking it seems like a good idea, let's
add values from Android and xcode using upstream format

Based on:
    drm/i915: add MOCS tables for SKL GT3e and GT4e for CL534416

Change-Id: Ie96bfb7b795d302093a8c2cc70b5b5b6026be321
Signed-off-by: Michał Winiarski <michal.winiarski@intel.com>
---
 drivers/gpu/drm/i915/intel_mocs.c |  178 ++++++++++++++++++++++++++++++++++++-
 1 files changed, 176 insertions(+), 2 deletions(-)

diff --git a/drivers/gpu/drm/i915/intel_mocs.c b/drivers/gpu/drm/i915/intel_mocs.c
index 6ba4bf7..57151be 100644
--- a/drivers/gpu/drm/i915/intel_mocs.c
+++ b/drivers/gpu/drm/i915/intel_mocs.c
@@ -107,7 +107,165 @@ static const struct drm_i915_mocs_entry skylake_mocs_table[] = {
 	/* { 0x0000003b, 0x0030 } */
 	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(LLC_ELLC) | LE_LRUM(3) |
 	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
-	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_WB)) }
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_WB)) },
+	/* { 0x00000037, 0x0030 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(LLC) | LE_LRUM(3) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_WB)) },
+	/* { 0x00000039, 0x0030 } */
+	{ (LE_CACHEABILITY(LE_UC) | LE_TGT_CACHE(LLC_ELLC) | LE_LRUM(3) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_WB)) },
+	/* { 0x00000037, 0x0010 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(LLC) | LE_LRUM(3) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_UC)) },
+	/* { 0x00000039, 0x0010 } */
+	{ (LE_CACHEABILITY(LE_UC) | LE_TGT_CACHE(LLC_ELLC) | LE_LRUM(3) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_UC)) },
+	/* { 0x00000017, 0x0010 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(LLC) | LE_LRUM(1) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_UC)) },
+	/* { 0x0000003b, 0x0010 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(LLC_ELLC) | LE_LRUM(3) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_UC)) },
+	/* { 0x00000033, 0x0030 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(ELLC) | LE_LRUM(3) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_WB)) },
+	/* { 0x00000033, 0x0010 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(ELLC) | LE_LRUM(3) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_UC)) },
+	/* { 0x00000017, 0x0030 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(LLC) | LE_LRUM(1) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_WB)) },
+	/* { 0x00000019, 0x0010 } */
+	{ (LE_CACHEABILITY(LE_UC) | LE_TGT_CACHE(LLC_ELLC) | LE_LRUM(1) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_UC)) }
+};
+
+static const struct drm_i915_mocs_entry skylake_gt3e_mocs_table[] = {
+	/* { 0x00000009, 0x0010 } */
+	{ (LE_CACHEABILITY(LE_UC) | LE_TGT_CACHE(LLC_ELLC) | LE_LRUM(0) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_UC)) },
+	/* { 0x00000038, 0x0030 } */
+	{ (LE_CACHEABILITY(LE_PAGETABLE) | LE_TGT_CACHE(LLC_ELLC) | LE_LRUM(3) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_WB)) },
+	/* { 0x0000003b, 0x0030 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(LLC_ELLC) | LE_LRUM(3) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_WB)) },
+	/* { 0x00000033, 0x0030 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(ELLC) | LE_LRUM(3) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_WB)) },
+	/* { 0x00000033, 0x0010 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(ELLC) | LE_LRUM(3) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_UC)) },
+	/* { 0x00000039, 0x0010 } */
+	{ (LE_CACHEABILITY(LE_UC) | LE_TGT_CACHE(LLC_ELLC) | LE_LRUM(3) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_UC)) },
+	/* { 0x00000013, 0x0010 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(ELLC) | LE_LRUM(1) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_UC)) },
+	/* { 0x0000003b, 0x0010 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(LLC_ELLC) | LE_LRUM(3) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_UC)) },
+	/* { 0x00000039, 0x0030 } */
+	{ (LE_CACHEABILITY(LE_UC) | LE_TGT_CACHE(LLC_ELLC) | LE_LRUM(3) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_WB)) },
+	/* { 0x00000037, 0x0030 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(LLC) | LE_LRUM(3) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_WB)) },
+	/* { 0x00000037, 0x0010 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(LLC) | LE_LRUM(3) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_UC)) },
+	/* { 0x0000001b, 0x0030 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(LLC_ELLC) | LE_LRUM(1) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_WB)) },
+	/* { 0x00000003, 0x0010 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(ELLC) | LE_LRUM(0) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_UC)) },
+	/* { 0x0000001b, 0x0010 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(LLC_ELLC) | LE_LRUM(1) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_UC)) }
+};
+
+static const struct drm_i915_mocs_entry skylake_gt4e_mocs_table[] = {
+	/* { 0x00000009, 0x0010 } */
+	{ (LE_CACHEABILITY(LE_UC) | LE_TGT_CACHE(LLC_ELLC) | LE_LRUM(0) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_UC)) },
+	/* { 0x00000038, 0x0030 } */
+	{ (LE_CACHEABILITY(LE_PAGETABLE) | LE_TGT_CACHE(LLC_ELLC) | LE_LRUM(3) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_WB)) },
+	/* { 0x0000003b, 0x0030 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(LLC_ELLC) | LE_LRUM(3) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_WB)) },
+	/* { 0x00000033, 0x0030 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(ELLC) | LE_LRUM(3) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_WB)) },
+	/* { 0x0000003b, 0x0010 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(LLC_ELLC) | LE_LRUM(3) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_UC)) },
+	/* { 0x00000039, 0x0010 } */
+	{ (LE_CACHEABILITY(LE_UC) | LE_TGT_CACHE(LLC_ELLC) | LE_LRUM(3) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_UC)) },
+	/* { 0x00000033, 0x0010 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(ELLC) | LE_LRUM(3) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_UC)) },
+	/* { 0x0000001b, 0x0010 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(LLC_ELLC) | LE_LRUM(1) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_UC)) },
+	/* { 0x00000039, 0x0030 } */
+	{ (LE_CACHEABILITY(LE_UC) | LE_TGT_CACHE(LLC_ELLC) | LE_LRUM(3) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_WB)) },
+	/* { 0x00000037, 0x0030 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(LLC) | LE_LRUM(3) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_WB)) },
+	/* { 0x00000037, 0x0010 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(LLC) | LE_LRUM(3) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_UC)) },
+	/* { 0x0000001b, 0x0030 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(LLC_ELLC) | LE_LRUM(1) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_WB)) },
+	/* { 0x00000003, 0x0010 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(ELLC) | LE_LRUM(0) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_UC)) },
+	/* { 0x00000013, 0x0010 } */
+	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(ELLC) | LE_LRUM(1) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_UC)) }
 };
 
 /* NOTE: the LE_TGT_CACHE is not used on Broxton */
@@ -123,6 +281,14 @@ static const struct drm_i915_mocs_entry broxton_mocs_table[] = {
 	/* { 0x0000003b, 0x0030 } */
 	{ (LE_CACHEABILITY(LE_WB) | LE_TGT_CACHE(LLC_ELLC) | LE_LRUM(3) |
 	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_WB)) },
+	/* { 0x00000005, 0x0010 } */
+	{ (LE_CACHEABILITY(LE_UC) | LE_TGT_CACHE(LLC) | LE_LRUM(0) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
+	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_UC)) },
+	/* { 0x00000005, 0x0030 } */
+	{ (LE_CACHEABILITY(LE_UC) | LE_TGT_CACHE(LLC) | LE_LRUM(0) |
+	   LE_AOM(0) | LE_RSC(0) | LE_SCC(0) | LE_PFM(0) | LE_SCF(0)),
 	  (L3_ESC(0) | L3_SCC(0) | L3_CACHEABILITY(L3_WB)) }
 };
 
@@ -143,7 +309,15 @@ static bool get_mocs_settings(struct drm_i915_private *dev_priv,
 {
 	bool result = false;
 
-	if (IS_SKYLAKE(dev_priv) || IS_KABYLAKE(dev_priv)) {
+	if (IS_SKL_GT3(dev_priv) && intel_uncore_edram_size(dev_priv)) {
+		table->size  = ARRAY_SIZE(skylake_gt3e_mocs_table);
+		table->table = skylake_gt3e_mocs_table;
+		result = true;
+	} else if (IS_SKL_GT4(dev_priv) && intel_uncore_edram_size(dev_priv)) {
+		table->size  = ARRAY_SIZE(skylake_gt4e_mocs_table);
+		table->table = skylake_gt4e_mocs_table;
+		result = true;
+	} else if (IS_SKYLAKE(dev_priv) || IS_KABYLAKE(dev_priv)) {
 		table->size  = ARRAY_SIZE(skylake_mocs_table);
 		table->table = skylake_mocs_table;
 		result = true;
-- 
1.7.1


From fba773ed61745872bc89d95d97074677a6d2091d Mon Sep 17 00:00:00 2001
From: Artur Harasimiuk <artur.harasimiuk@intel.com>
Date: Fri, 8 Jan 2016 15:12:34 +0100
Subject: [PATCH 03/17] drm/i915: Exec flag to force non IA-Coherent cache for Gen9+
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Starting from Gen9 we can use IA-Coherent caches. Generally, coherency
can be programmed using RENDER_SURFACE_STATE or BTI 255, depending if
surface state model or stateless model is used. It is important to control
whether IA or GPU cache coherency should be used, especially for non-LLC
devices. However this control is complicated when stateless memory access
model is in action. It would require dedicated ISA code depending on
coherency requirement.

By setting HDC_FORCE_NON_COHERENT we *Force* data port to ignore these
attributes and all caches are GPU-Coherent. This register is part of HW
context, however it is private and cannot be programmed from
non-privileged batch buffer.

Default operation mode is as programmed by workaround. When
WaForceEnableNonCoherent is in place caches are GPU-Coherent and we
should not change it back to IA-Coherent because this can lead to GPU
hangs (as workaround description says).

A new device parameter is to inform user space about kernel capability.
It tells if can request to disable IA-Coherency.

Exec flag is to allow UMD to decide whether IA-Coherency is not needed
for submitted batch buffer. Exec flag behavior:
    1. flag is not set - use system default
    2. flag is set but WaForceEnableNonCoherent is
       a) not programmed - *Force* GPU-Coherent cache by setting
          HDC_FORCE_NON_COHERENT prior to bb_start and clearing after
       b) programmed - do nothing, GPU-Coherent is already in place

v2: Ringbufer handling fixes (Chris)
    Moved workarounds to common place (Chris)
    Removed flag cleanup (Dave)
    Updated commit message to reflect comments (Chris,Dave)
v3 (private): Move ABI exposed to userspace to avoid conflicts with upstream

Change-Id: Ifee222bb6a0c08a97323b9fbb0529669309d2a66
Signed-off-by: Artur Harasimiuk <artur.harasimiuk@intel.com>
Signed-off-by: Michał Winiarski <michal.winiarski@intel.com>

Conflicts:
	drivers/gpu/drm/i915/intel_lrc.c
	include/uapi/drm/i915_drm.h
---
 drivers/gpu/drm/i915/i915_dma.c            |    4 +++
 drivers/gpu/drm/i915/i915_drv.h            |    4 +++
 drivers/gpu/drm/i915/i915_gem_execbuffer.c |    4 +++
 drivers/gpu/drm/i915/intel_lrc.c           |   38 ++++++++++++++++++++++++++++
 drivers/gpu/drm/i915/intel_ringbuffer.c    |    2 +
 include/uapi/drm/i915_drm.h                |   10 ++++++-
 6 files changed, 61 insertions(+), 1 deletions(-)

diff --git a/drivers/gpu/drm/i915/i915_dma.c b/drivers/gpu/drm/i915/i915_dma.c
index b3198fc..c85fdcc 100644
--- a/drivers/gpu/drm/i915/i915_dma.c
+++ b/drivers/gpu/drm/i915/i915_dma.c
@@ -232,6 +232,10 @@ static int i915_getparam(struct drm_device *dev, void *data,
 	case I915_PARAM_HAS_EXEC_SOFTPIN:
 		value = 1;
 		break;
+	case I915_PRIVATE_PARAM_HAS_EXEC_FORCE_NON_COHERENT:
+		value = !dev_priv->workarounds.WaForceEnableNonCoherent &&
+			INTEL_INFO(dev)->gen >= 9;
+		break;
 	default:
 		DRM_DEBUG("Unknown parameter %d\n", param->param);
 		return -EINVAL;
diff --git a/drivers/gpu/drm/i915/i915_drv.h b/drivers/gpu/drm/i915/i915_drv.h
index bc3f2e6..26d6a27 100644
--- a/drivers/gpu/drm/i915/i915_drv.h
+++ b/drivers/gpu/drm/i915/i915_drv.h
@@ -1684,6 +1684,10 @@ struct i915_workarounds {
 	struct i915_wa_reg reg[I915_MAX_WA_REGS];
 	u32 count;
 	u32 hw_whitelist_count[I915_NUM_ENGINES];
+
+	struct {
+		unsigned int WaForceEnableNonCoherent:1;
+	};
 };
 
 struct i915_virtual_gpu {
diff --git a/drivers/gpu/drm/i915/i915_gem_execbuffer.c b/drivers/gpu/drm/i915/i915_gem_execbuffer.c
index 33df74d..4629eda 100644
--- a/drivers/gpu/drm/i915/i915_gem_execbuffer.c
+++ b/drivers/gpu/drm/i915/i915_gem_execbuffer.c
@@ -1448,6 +1448,10 @@ i915_gem_do_execbuffer(struct drm_device *dev, void *data,
 	if (!i915_gem_check_execbuffer(args))
 		return -EINVAL;
 
+	if ((args->flags & I915_PRIVATE_EXEC_FORCE_NON_COHERENT) &&
+		INTEL_INFO(dev)->gen < 9)
+		return -EINVAL;
+
 	ret = validate_exec_list(dev, exec, args->buffer_count);
 	if (ret)
 		return ret;
diff --git a/drivers/gpu/drm/i915/intel_lrc.c b/drivers/gpu/drm/i915/intel_lrc.c
index 7f2d841..bb49260 100644
--- a/drivers/gpu/drm/i915/intel_lrc.c
+++ b/drivers/gpu/drm/i915/intel_lrc.c
@@ -787,6 +787,36 @@ int intel_logical_ring_reserve_space(struct drm_i915_gem_request *request)
 	return intel_ring_begin(request, 0);
 }
 
+static inline int
+intel_lr_emit_force_non_coherent(struct i915_execbuffer_params *params,
+				 struct drm_i915_gem_execbuffer2 *args,
+				 bool force)
+{
+	struct drm_i915_private *dev_priv = params->dev->dev_private;
+	int ret;
+
+	if (dev_priv->workarounds.WaForceEnableNonCoherent)
+		return 0;
+
+	if (args->flags & I915_PRIVATE_EXEC_FORCE_NON_COHERENT) {
+		struct intel_ringbuffer *ringbuf = params->request->ringbuf;
+
+		ret = intel_ring_begin(params->request, 4);
+		if (ret)
+			return ret;
+
+		intel_logical_ring_emit(ringbuf, MI_NOOP);
+		intel_logical_ring_emit(ringbuf, MI_LOAD_REGISTER_IMM(1));
+		intel_logical_ring_emit_reg(ringbuf, HDC_CHICKEN0);
+		intel_logical_ring_emit(ringbuf, force ?
+				_MASKED_BIT_ENABLE(HDC_FORCE_NON_COHERENT) :
+				_MASKED_BIT_DISABLE(HDC_FORCE_NON_COHERENT));
+		intel_logical_ring_advance(ringbuf);
+	}
+
+	return 0;
+}
+
 /**
  * execlists_submission() - submit a batchbuffer for execution, Execlists style
  * @dev: DRM device.
@@ -867,6 +897,10 @@ int intel_execlists_submission(struct i915_execbuffer_params *params,
 		dev_priv->relative_constants_mode = instp_mode;
 	}
 
+	ret = intel_lr_emit_force_non_coherent(params, args, true);
+	if (ret)
+		return ret;
+
 	exec_start = params->batch_obj_vm_offset +
 		     args->batch_start_offset;
 
@@ -874,6 +908,10 @@ int intel_execlists_submission(struct i915_execbuffer_params *params,
 	if (ret)
 		return ret;
 
+	ret = intel_lr_emit_force_non_coherent(params, args, false);
+	if (ret)
+		return ret;
+
 	trace_i915_gem_ring_dispatch(params->request, params->dispatch_flags);
 
 	i915_gem_execbuffer_move_to_active(vmas, params->request);
diff --git a/drivers/gpu/drm/i915/intel_ringbuffer.c b/drivers/gpu/drm/i915/intel_ringbuffer.c
index 68c5af0..6caf442 100644
--- a/drivers/gpu/drm/i915/intel_ringbuffer.c
+++ b/drivers/gpu/drm/i915/intel_ringbuffer.c
@@ -828,6 +828,7 @@ static int gen8_init_workarounds(struct intel_engine_cs *engine)
 	 * invalidation occurs during a PSD flush.
 	 */
 	/* WaForceEnableNonCoherent:bdw,chv */
+	dev_priv->workarounds.WaForceEnableNonCoherent = 1;
 	/* WaHdcDisableFetchWhenMasked:bdw,chv */
 	WA_SET_BIT_MASKED(HDC_CHICKEN0,
 			  HDC_DONOT_FETCH_MEM_WHEN_MASKED |
@@ -994,6 +995,7 @@ static int gen9_init_workarounds(struct intel_engine_cs *engine)
 	 */
 
 	/* WaForceEnableNonCoherent:skl,bxt,kbl */
+	dev_priv->workarounds.WaForceEnableNonCoherent = 1;
 	WA_SET_BIT_MASKED(HDC_CHICKEN0,
 			  HDC_FORCE_NON_COHERENT);
 
diff --git a/include/uapi/drm/i915_drm.h b/include/uapi/drm/i915_drm.h
index c17d63d..2843d5e 100644
--- a/include/uapi/drm/i915_drm.h
+++ b/include/uapi/drm/i915_drm.h
@@ -362,6 +362,8 @@ typedef struct drm_i915_irq_wait {
 #define I915_PARAM_HAS_RESOURCE_STREAMER 36
 #define I915_PARAM_HAS_EXEC_SOFTPIN	 37
 
+#define I915_PRIVATE_PARAM_HAS_EXEC_FORCE_NON_COHERENT (-1)
+
 typedef struct drm_i915_getparam {
 	__s32 param;
 	/*
@@ -788,7 +790,13 @@ struct drm_i915_gem_execbuffer2 {
  */
 #define I915_EXEC_RESOURCE_STREAMER     (1<<15)
 
-#define __I915_EXEC_UNKNOWN_FLAGS -(I915_EXEC_RESOURCE_STREAMER<<1)
+#define __I915_EXEC_UNKNOWN_FLAGS (-(I915_EXEC_RESOURCE_STREAMER<<1) & \
+				   ~I915_PRIVATE_EXEC_FORCE_NON_COHERENT)
+
+/**
+ * Tell the kernel that the batch buffer requires to disable IA-Coherency
+ */
+#define I915_PRIVATE_EXEC_FORCE_NON_COHERENT    (1<<31)
 
 #define I915_EXEC_CONTEXT_ID_MASK	(0xffffffff)
 #define i915_execbuffer2_set_context_id(eb2, context) \
-- 
1.7.1


From ae42cb5b6b7c0296414a9190162d99d0fb0929f4 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Micha=C5=82=20Winiarski?= <michal.winiarski@intel.com>
Date: Mon, 8 Aug 2016 12:04:30 +0200
Subject: [PATCH 04/17] Revert "drm/i915/skl: Fix spurious gpu hang with gt3/gt4 revs"

This reverts commit 97ea6be161c55dec896b65c95157d953c330ae05.
Also partially revert 60f452e61, since we're not doing
WaForceEnableNonCoherent for kbl.

Change-Id: Ibf517517522f31b268710b9286b93cdc307d56b1
---
 drivers/gpu/drm/i915/intel_ringbuffer.c |   30 ++++++++++++------------------
 1 files changed, 12 insertions(+), 18 deletions(-)

diff --git a/drivers/gpu/drm/i915/intel_ringbuffer.c b/drivers/gpu/drm/i915/intel_ringbuffer.c
index 6caf442..33e9de1 100644
--- a/drivers/gpu/drm/i915/intel_ringbuffer.c
+++ b/drivers/gpu/drm/i915/intel_ringbuffer.c
@@ -981,24 +981,6 @@ static int gen9_init_workarounds(struct intel_engine_cs *engine)
 			  HDC_FORCE_CONTEXT_SAVE_RESTORE_NON_COHERENT |
 			  HDC_FORCE_CSR_NON_COHERENT_OVR_DISABLE);
 
-	/* WaForceEnableNonCoherent and WaDisableHDCInvalidation are
-	 * both tied to WaForceContextSaveRestoreNonCoherent
-	 * in some hsds for skl. We keep the tie for all gen9. The
-	 * documentation is a bit hazy and so we want to get common behaviour,
-	 * even though there is no clear evidence we would need both on kbl/bxt.
-	 * This area has been source of system hangs so we play it safe
-	 * and mimic the skl regardless of what bspec says.
-	 *
-	 * Use Force Non-Coherent whenever executing a 3D context. This
-	 * is a workaround for a possible hang in the unlikely event
-	 * a TLB invalidation occurs during a PSD flush.
-	 */
-
-	/* WaForceEnableNonCoherent:skl,bxt,kbl */
-	dev_priv->workarounds.WaForceEnableNonCoherent = 1;
-	WA_SET_BIT_MASKED(HDC_CHICKEN0,
-			  HDC_FORCE_NON_COHERENT);
-
 	/* WaDisableHDCInvalidation:skl,bxt,kbl */
 	I915_WRITE(GAM_ECOCHK, I915_READ(GAM_ECOCHK) |
 		   BDW_DISABLE_HDC_INVALIDATION);
@@ -1122,6 +1104,18 @@ static int skl_init_workarounds(struct intel_engine_cs *engine)
 		WA_SET_BIT_MASKED(HIZ_CHICKEN,
 				  BDW_HIZ_POWER_COMPILER_CLOCK_GATING_DISABLE);
 
+	if (IS_SKL_REVID(dev, 0, SKL_REVID_F0)) {
+		/*
+		 *Use Force Non-Coherent whenever executing a 3D context. This
+		 * is a workaround for a possible hang in the unlikely event
+		 * a TLB invalidation occurs during a PSD flush.
+		 */
+		/* WaForceEnableNonCoherent:skl */
+		dev_priv->workarounds.WaForceEnableNonCoherent = 1;
+		WA_SET_BIT_MASKED(HDC_CHICKEN0,
+				  HDC_FORCE_NON_COHERENT);
+	}
+
 	/* WaBarrierPerformanceFixDisable:skl */
 	if (IS_SKL_REVID(dev, SKL_REVID_C0, SKL_REVID_D0))
 		WA_SET_BIT_MASKED(HDC_CHICKEN0,
-- 
1.7.1


From 2448620afd57dfc16afa201c126fe1419ae744d1 Mon Sep 17 00:00:00 2001
From: Woo, Insoo <insoo.woo@intel.com>
Date: Thu, 18 Feb 2016 09:20:52 -0800
Subject: [PATCH 05/17] drm/i915: OA regs configuration for MDAPI TBS

This patch implements Perfmon IOCTL entry points for MDAPI.
It is originally from MDAPI for Android.

Perfmon open/close IOCTL
Perfmon must be opened in order to set global perfmon
configuration and to get list of HW context IDs for arbitrary
context.

Reporting IOCTL of HW context
Enables user to get hardware context id for given context
(identified by user handle) as well as to query for list
of hardware context ids for arbitrary process (identified
by PID). This is needed for matching Perfmon sample from HW
with application/process/context. Perfmon must be opened
for given fd to enable caller to get list of HW context
IDs for arbitrary context.

Writing OA / Perfmon configuration to ring buffer
Some registers for OA configuration are part of context and
thus need to be written via LRIs inserted to the ring buffer.
It is convenient to send all OA configuration registers this way
since it enables us to implement multiconfiguration, meaning
multiple users each using different OA config. To have this
working programming of per-context workaround batch buffers
is required.

Change-Id: I5bc2922b1d10ca5077990d7660e47294b22dc052
Signed-off-by: Woo, Insoo <insoo.woo@intel.com>
(cherry picked from commit 99ba7d56a954ed7d44d170d290d25296ec548cdf)

Conflicts:
	drivers/gpu/drm/i915/i915_dma.c
	drivers/gpu/drm/i915/i915_drv.h
	drivers/gpu/drm/i915/i915_reg.h
	drivers/gpu/drm/i915/intel_lrc.c
	include/uapi/drm/i915_drm.h
---
 drivers/gpu/drm/i915/Makefile            |    3 +
 drivers/gpu/drm/i915/i915_dma.c          |   11 +
 drivers/gpu/drm/i915/i915_drv.h          |   26 +
 drivers/gpu/drm/i915/i915_gem_context.c  |    7 +
 drivers/gpu/drm/i915/i915_perfmon.c      | 1067 ++++++++++++++++++++++++++++++
 drivers/gpu/drm/i915/i915_perfmon_defs.h |   62 ++
 drivers/gpu/drm/i915/i915_reg.h          |    3 +
 drivers/gpu/drm/i915/intel_lrc.c         |    7 +
 include/uapi/drm/i915_drm.h              |    4 +
 include/uapi/drm/i915_perfmon.h          |  125 ++++
 10 files changed, 1315 insertions(+), 0 deletions(-)
 create mode 100644 drivers/gpu/drm/i915/i915_perfmon.c
 create mode 100644 drivers/gpu/drm/i915/i915_perfmon_defs.h
 create mode 100644 include/uapi/drm/i915_perfmon.h

diff --git a/drivers/gpu/drm/i915/Makefile b/drivers/gpu/drm/i915/Makefile
index 0b88ba0..0cff69c 100644
--- a/drivers/gpu/drm/i915/Makefile
+++ b/drivers/gpu/drm/i915/Makefile
@@ -95,6 +95,9 @@ i915-y += dvo_ch7017.o \
 	  intel_sdvo.o \
 	  intel_tv.o
 
+# performance monitoring
+i915-y += i915_perfmon.o
+
 # virtual gpu code
 i915-y += i915_vgpu.o
 
diff --git a/drivers/gpu/drm/i915/i915_dma.c b/drivers/gpu/drm/i915/i915_dma.c
index c85fdcc..5be83cc 100644
--- a/drivers/gpu/drm/i915/i915_dma.c
+++ b/drivers/gpu/drm/i915/i915_dma.c
@@ -1043,6 +1043,9 @@ static int i915_driver_init_early(struct drm_i915_private *dev_priv,
 	/* This must be called before any calls to HAS_PCH_* */
 	intel_detect_pch(dev);
 
+	mutex_init(&dev_priv->perfmon.config.lock);
+	mutex_init(&dev_priv->rc6_wa_bb.lock);
+
 	intel_pm_setup(dev);
 	intel_init_dpio(dev_priv);
 	intel_power_domains_init(dev_priv);
@@ -1319,6 +1322,10 @@ static void i915_driver_register(struct drm_i915_private *dev_priv)
 		intel_gpu_ips_init(dev_priv);
 
 	i915_audio_component_init(dev_priv);
+
+	/* Initialize all the resource for perf monitoring */
+	i915_perfmon_setup(dev_priv);
+	i915_perfmon_cleanup(dev_priv);
 }
 
 /**
@@ -1477,6 +1484,9 @@ int i915_driver_unload(struct drm_device *dev)
 	intel_display_power_put(dev_priv, POWER_DOMAIN_INIT);
 
 	i915_driver_cleanup_early(dev_priv);
+
+	/* clean up all the resource for perf monitoring */
+	i915_perfmon_cleanup(dev_priv);
 	kfree(dev_priv);
 
 	return 0;
@@ -1586,6 +1596,7 @@ const struct drm_ioctl_desc i915_ioctls[] = {
 	DRM_IOCTL_DEF_DRV(I915_GEM_USERPTR, i915_gem_userptr_ioctl, DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(I915_GEM_CONTEXT_GETPARAM, i915_gem_context_getparam_ioctl, DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(I915_GEM_CONTEXT_SETPARAM, i915_gem_context_setparam_ioctl, DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(I915_PERFMON, i915_perfmon_ioctl, DRM_UNLOCKED),
 };
 
 int i915_max_ioctl = ARRAY_SIZE(i915_ioctls);
diff --git a/drivers/gpu/drm/i915/i915_drv.h b/drivers/gpu/drm/i915/i915_drv.h
index 26d6a27..d425a70 100644
--- a/drivers/gpu/drm/i915/i915_drv.h
+++ b/drivers/gpu/drm/i915/i915_drv.h
@@ -61,6 +61,8 @@
 #include "i915_gem_gtt.h"
 #include "i915_gem_render_state.h"
 
+#include  "i915_perfmon_defs.h"
+
 /* General customization:
  */
 
@@ -381,6 +383,7 @@ struct drm_i915_file_private {
 	} rps;
 
 	unsigned int bsd_ring;
+	struct drm_i915_perfmon_file perfmon;
 };
 
 /* Used by dp and fdi links */
@@ -843,6 +846,7 @@ struct i915_ctx_hang_stats {
  */
 struct intel_context {
 	struct kref ref;
+        struct pid *pid;
 	int user_handle;
 	uint8_t remap_slice;
 	struct drm_i915_private *i915;
@@ -868,6 +872,9 @@ struct intel_context {
 	} engine[I915_NUM_ENGINES];
 
 	struct list_head link;
+
+	/* perfmon configuration */
+	struct drm_i915_perfmon_context perfmon;
 };
 
 enum fb_op_origin {
@@ -1872,6 +1879,16 @@ struct drm_i915_private {
 
 	struct i915_workarounds workarounds;
 
+	struct drm_i915_perfmon_device perfmon;
+
+	struct {
+		struct drm_i915_gem_object *obj;
+		unsigned long offset;
+		void *address;
+		atomic_t enable;
+		struct mutex lock;
+	} rc6_wa_bb;
+
 	struct i915_frontbuffer_tracking fb_tracking;
 
 	u16 orig_clock;
@@ -3569,6 +3586,15 @@ int i915_reg_read_ioctl(struct drm_device *dev, void *data,
 int i915_get_reset_stats_ioctl(struct drm_device *dev, void *data,
 			       struct drm_file *file);
 
+/* i915_perfmon.c */
+int i915_perfmon_ioctl(struct drm_device *dev, void *data, struct drm_file *file);
+void i915_perfmon_setup(struct drm_i915_private *dev_priv);
+void i915_perfmon_cleanup(struct drm_i915_private *dev_priv);
+void i915_perfmon_ctx_setup(struct intel_context *ctx);
+void i915_perfmon_ctx_cleanup(struct intel_context *ctx);
+int i915_perfmon_update_workaround_bb(struct drm_i915_private *dev_priv,
+				      struct drm_i915_perfmon_config *config);
+
 /* overlay */
 extern struct intel_overlay_error_state *intel_overlay_capture_error_state(struct drm_device *dev);
 extern void intel_overlay_print_error_state(struct drm_i915_error_state_buf *e,
diff --git a/drivers/gpu/drm/i915/i915_gem_context.c b/drivers/gpu/drm/i915/i915_gem_context.c
index e5acc39..a8aeec9 100644
--- a/drivers/gpu/drm/i915/i915_gem_context.c
+++ b/drivers/gpu/drm/i915/i915_gem_context.c
@@ -168,6 +168,10 @@ void i915_gem_context_free(struct kref *ctx_ref)
 
 	if (ctx->legacy_hw_ctx.rcs_state)
 		drm_gem_object_unreference(&ctx->legacy_hw_ctx.rcs_state->base);
+
+	put_pid(ctx->pid);
+	i915_perfmon_ctx_cleanup(ctx);
+
 	list_del(&ctx->link);
 	kfree(ctx);
 }
@@ -310,6 +314,9 @@ i915_gem_create_context(struct drm_device *dev,
 
 	trace_i915_context_create(ctx);
 
+	ctx->pid = get_pid(task_tgid(current));
+	i915_perfmon_ctx_setup(ctx);
+
 	return ctx;
 
 err_unpin:
diff --git a/drivers/gpu/drm/i915/i915_perfmon.c b/drivers/gpu/drm/i915/i915_perfmon.c
new file mode 100644
index 0000000..fb3df80
--- /dev/null
+++ b/drivers/gpu/drm/i915/i915_perfmon.c
@@ -0,0 +1,1067 @@
+/*
+ * Copyright  2013 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ *
+ */
+#include <drm/i915_drm.h>
+#include "i915_drv.h"
+#include "i915_trace.h"
+#include "intel_drv.h"
+#include "linux/wait.h"
+#include "i915_perfmon_defs.h"
+
+/**
+ * i915_get_render_hw_ctx_id
+ *
+ * Get render engine HW context ID for given context. This is the
+ * representation of context in the HW. This is *not* context ID as referenced
+ * by usermode. For legacy submission path this is logical ring context address.
+ * For execlist this is the kernel managed context ID written to execlist
+ * descriptor.
+ */
+static int  i915_get_render_hw_ctx_id(
+	struct drm_i915_private *dev_priv,
+	struct intel_context *ctx,
+	__u32 *id)
+{
+	struct drm_i915_gem_object *ctx_obj =
+		ctx->engine[RCS].state;
+
+	if (!ctx_obj)
+		return -ENOENT;
+
+	*id = i915.enable_execlists ?
+			intel_execlists_ctx_id(ctx, &dev_priv->engine[RCS] ) :
+			i915_gem_obj_ggtt_offset(ctx_obj) >> 12;
+	return 0;
+}
+
+/**
+ * i915_perfmon_get_hw_ctx_id
+ *
+ * Get HW context ID for given context ID and DRM file.
+ */
+static int i915_perfmon_get_hw_ctx_id(
+	struct drm_device *dev,
+	struct drm_file *file,
+	struct drm_i915_perfmon_get_hw_ctx_id *ioctl_data)
+{
+	struct drm_i915_private *dev_priv =
+		(struct drm_i915_private *) dev->dev_private;
+	struct intel_context *ctx;
+	struct drm_i915_file_private *file_priv = file->driver_priv;
+	int ret;
+
+	if (!HAS_HW_CONTEXTS(dev))
+		return -ENODEV;
+
+	ret = i915_mutex_lock_interruptible(dev);
+	if (ret)
+		return ret;
+
+	ctx = i915_gem_context_get(file_priv, ioctl_data->ctx_id);
+	if (IS_ERR_OR_NULL(ctx))
+		ret = -ENOENT;
+	else
+		ret = i915_get_render_hw_ctx_id(dev_priv, ctx,
+			&ioctl_data->hw_ctx_id);
+
+	mutex_unlock(&dev->struct_mutex);
+
+	return ret;
+}
+
+struct i915_perfmon_hw_ctx_list {
+	__u32 *ids;
+	__u32 capacity;
+	__u32 size;
+	__u32 iterations_left;
+};
+
+/**
+ * process_context
+ *
+ * Check if context referenced by 'ptr' belongs to application with
+ * provided process ID. If so, increment total number of contexts
+ * found (list->size) and add context id to the list if
+ * its capacity is not reached.
+ */
+static int process_context(struct drm_i915_private *dev_priv,
+	struct intel_context *ctx,
+	__u32 pid,
+	struct i915_perfmon_hw_ctx_list *list)
+{
+	bool ctx_match;
+	bool has_render_ring;
+	__u32 id;
+
+	if (list->iterations_left == 0)
+		return 0;
+	--list->iterations_left;
+
+	ctx_match = (pid == pid_vnr(ctx->pid) ||
+			 pid == 0 ||
+			 ctx == dev_priv->kernel_context);
+
+	if (ctx_match) {
+		has_render_ring =
+			(0 == i915_get_render_hw_ctx_id(
+				dev_priv, ctx, &id));
+	}
+
+	if (ctx_match && has_render_ring) {
+		if (list->size < list->capacity)
+			list->ids[list->size] = id;
+		list->size++;
+	}
+
+	return 0;
+}
+
+/**
+ * i915_perfmon_get_hw_ctx_ids
+ *
+ * Lookup the list of all contexts and return HW context IDs of those
+ * belonging to provided process id.
+ *
+ * User specifies maximum number of IDs to be written to provided block of
+ * memory: ioctl_data->count. Returned is the list of not more than
+ * ioctl_data->count HW context IDs together with total number of matching
+ * contexts found - potentially more than ioctl_data->count.
+ *
+ */
+static int i915_perfmon_get_hw_ctx_ids(
+	struct drm_device *dev,
+	struct drm_i915_perfmon_get_hw_ctx_ids *ioctl_data)
+{
+	struct drm_i915_private *dev_priv =
+		(struct drm_i915_private *) dev->dev_private;
+	struct i915_perfmon_hw_ctx_list list;
+	struct intel_context *ctx;
+	unsigned int ids_to_copy;
+	int ret;
+
+	if (!HAS_HW_CONTEXTS(dev))
+		return -ENODEV;
+
+	if (ioctl_data->count > I915_PERFMON_MAX_HW_CTX_IDS)
+		return -EINVAL;
+
+	list.ids = kzalloc(
+		ioctl_data->count * sizeof(__u32), GFP_KERNEL);
+	if (!list.ids)
+		return -ENOMEM;
+	list.capacity = ioctl_data->count;
+	list.size = 0;
+	list.iterations_left = I915_PERFMON_MAX_HW_CTX_IDS;
+
+	ret = i915_mutex_lock_interruptible(dev);
+	if (ret)
+		goto exit;
+
+	list_for_each_entry(ctx, &dev_priv->context_list, link) {
+		process_context(dev_priv, ctx, ioctl_data->pid, &list);
+	}
+
+	mutex_unlock(&dev->struct_mutex);
+
+	/*
+	 * After we searched all the contexts list.size is the total number
+	 * of contexts matching the query. This is potentially more than
+	 * the capacity of user buffer (list.capacity).
+	 */
+	ids_to_copy = min(list.size, list.capacity);
+	if (copy_to_user(
+		(__u32 __user *)(uintptr_t)ioctl_data->ids,
+		list.ids,
+		ids_to_copy * sizeof(__u32))) {
+		ret = -EFAULT;
+		goto exit;
+	}
+
+	/* Return total number of matching ids to the user. */
+	ioctl_data->count = list.size;
+
+exit:
+	kfree(list.ids);
+	return ret;
+}
+
+/**
+ * copy_entries
+ *
+ * Helper function to copy OA configuration entries to new destination.
+ *
+ * Source configuration is first validated. In case of success pointer to newly
+ * allocated memory containing copy of source configuration is returned in *out.
+ *
+ */
+static int copy_entries(
+	struct drm_i915_perfmon_config *source,
+	bool user,
+	void **out)
+{
+	size_t size = 0;
+
+	*out = NULL;
+
+	/* basic validation of input */
+	if (source->id == 0 || source->size == 0 || source->entries == NULL)
+		return 0;
+
+	if (source->size > I915_PERFMON_CONFIG_SIZE)
+		return -EINVAL;
+
+	size = source->size  * sizeof(struct drm_i915_perfmon_config_entry);
+
+	*out = kzalloc(
+		   size,
+		   GFP_KERNEL);
+	if (*out == NULL) {
+		//DRM_ERROR("failed to allocate configuration buffer\n");
+		printk("failed to allocate configuration buffer\n");
+		return -ENOMEM;
+	}
+
+	if (user) {
+		int ret = copy_from_user(*out, source->entries, size);
+		if (ret) {
+			DRM_ERROR("failed to copy user provided config: %x\n",
+					ret);
+			kfree(*out);
+			*out = NULL;
+			return -EFAULT;
+		}
+	} else
+		memcpy(*out, source->entries, size);
+
+	return 0;
+}
+
+/**
+ * i915_perfmon_copy_config
+ *
+ * Utility function to copy OA and GP configuration to its destination.
+ *
+ * This is first used when global configuration is set by the user by calling
+ * I915_PERFMON_SET_CONFIG and then for the second time (optionally) when user
+ * calls I915_PERFMON_LOAD_CONFIG to copy the configuration from global storage
+ * to his context.
+ *
+ * 'user' boolean value indicates whether pointer to source config is provided
+ * by usermode (I915_PERFMON_SET_CONFIG case).
+ *
+ * If both OA and GP config are provided (!= NULL) then either both are copied
+ * to their respective locations or none of them (which is indicated by return
+ * value != 0).
+ *
+ * target_oa and target_gp are assumed to be non-NULL.
+ *
+ */
+static int i915_perfmon_copy_config(
+	struct drm_i915_private *dev_priv,
+	struct drm_i915_perfmon_config *target_oa,
+	struct drm_i915_perfmon_config *target_gp,
+	struct drm_i915_perfmon_config source_oa,
+	struct drm_i915_perfmon_config source_gp,
+	bool user)
+{
+	void *temp_oa = NULL;
+	void *temp_gp = NULL;
+	int ret = 0;
+
+	BUG_ON(!mutex_is_locked(&dev_priv->perfmon.config.lock));
+
+	/* copy configurations to temporary storage */
+	ret = copy_entries(&source_oa, user, &temp_oa);
+	if (ret)
+		return ret;
+	ret = copy_entries(&source_gp, user, &temp_gp);
+	if (ret) {
+		kfree(temp_oa);
+		return ret;
+	}
+
+	/*
+	 * Allocation and copy successful, free old config memory and swap
+	 * pointers
+	 */
+	if (temp_oa) {
+		kfree(target_oa->entries);
+		target_oa->entries = temp_oa;
+		target_oa->id = source_oa.id;
+		target_oa->size = source_oa.size;
+	}
+	if (temp_gp) {
+		kfree(target_gp->entries);
+		target_gp->entries = temp_gp;
+		target_gp->id = source_gp.id;
+		target_gp->size = source_gp.size;
+	}
+
+	return 0;
+}
+
+/**
+ * i915_perfmon_set_config
+ *
+ * Store OA/GP configuration for later use.
+ *
+ * Configuration content is not validated since it is provided by user who had
+ * previously called Perfmon Open with sysadmin privilege level.
+ *
+ */
+static int i915_perfmon_set_config(
+	struct drm_device *dev,
+	struct drm_i915_perfmon_set_config *args)
+{
+	struct drm_i915_private *dev_priv =
+		(struct drm_i915_private *) dev->dev_private;
+	int ret = 0;
+
+	struct drm_i915_perfmon_config user_config_oa;
+	struct drm_i915_perfmon_config user_config_gp;
+
+	if (!(IS_GEN8(dev)) && !(IS_GEN9(dev)))
+		return -EINVAL;
+
+	/* validate target */
+	switch (args->target) {
+	case I915_PERFMON_CONFIG_TARGET_CTX:
+	case I915_PERFMON_CONFIG_TARGET_PID:
+	case I915_PERFMON_CONFIG_TARGET_ALL:
+		/* OK */
+		break;
+	default:
+		//DRM_DEBUG("invalid target\n");
+		printk("invalid target\n");
+		return -EINVAL;
+	}
+
+	/* setup input for i915_perfmon_copy_config */
+	user_config_oa.id = args->oa.id;
+	user_config_oa.size = args->oa.size;
+	user_config_oa.entries =
+		(struct drm_i915_perfmon_config_entry __user *)
+			(uintptr_t)args->oa.entries;
+
+	user_config_gp.id = args->gp.id;
+	user_config_gp.size = args->gp.size;
+	user_config_gp.entries =
+		(struct drm_i915_perfmon_config_entry __user *)
+			(uintptr_t)args->gp.entries;
+
+	ret = mutex_lock_interruptible(&dev_priv->perfmon.config.lock);
+	if (ret)
+		return ret;
+
+	if (!atomic_read(&dev_priv->perfmon.config.enable)) {
+		ret = -EINVAL;
+		goto unlock_perfmon;
+	}
+
+	ret = i915_perfmon_copy_config(dev_priv,
+			&dev_priv->perfmon.config.oa,
+			&dev_priv->perfmon.config.gp,
+			user_config_oa, user_config_gp,
+			true);
+
+	if (ret)
+		goto unlock_perfmon;
+
+	dev_priv->perfmon.config.target = args->target;
+	dev_priv->perfmon.config.pid = args->pid;
+
+
+unlock_perfmon:
+	mutex_unlock(&dev_priv->perfmon.config.lock);
+
+	return ret;
+}
+
+/**
+ * i915_perfmon_load_config
+ *
+ * Copy configuration from global storage to current context.
+ *
+ */
+static int i915_perfmon_load_config(
+	struct drm_device *dev,
+	struct drm_file *file,
+	struct drm_i915_perfmon_load_config *args)
+{
+	struct drm_i915_private *dev_priv =
+		(struct drm_i915_private *) dev->dev_private;
+	struct drm_i915_file_private *file_priv = file->driver_priv;
+	struct intel_context *ctx;
+	struct drm_i915_perfmon_config user_config_oa;
+	struct drm_i915_perfmon_config user_config_gp;
+	int ret;
+
+	if (!(IS_GEN8(dev)) && !(IS_GEN9(dev)))
+		return -EINVAL;
+
+	ret = i915_mutex_lock_interruptible(dev);
+	if (ret)
+		return ret;
+
+	if (!atomic_read(&dev_priv->perfmon.config.enable)) {
+		ret = -EINVAL;
+		goto unlock_dev;
+	}
+
+	ctx = i915_gem_context_get(
+				file_priv,
+				args->ctx_id);
+
+	if (IS_ERR_OR_NULL(ctx)) {
+		DRM_DEBUG("invalid context\n");
+		ret = -EINVAL;
+		goto unlock_dev;
+	}
+
+	ret = mutex_lock_interruptible(&dev_priv->perfmon.config.lock);
+	if (ret)
+		goto unlock_dev;
+
+	user_config_oa = dev_priv->perfmon.config.oa;
+	user_config_gp = dev_priv->perfmon.config.gp;
+
+	/*
+	 * copy configuration to the context only if requested config ID matches
+	 * device configuration ID
+	 */
+	if (!(args->oa_id != 0 &&
+	      args->oa_id == dev_priv->perfmon.config.oa.id))
+		user_config_oa.entries = NULL;
+	if (!(args->gp_id != 0 &&
+	     args->gp_id == dev_priv->perfmon.config.gp.id))
+		user_config_gp.entries = NULL;
+
+	ret = i915_perfmon_copy_config(dev_priv,
+			&ctx->perfmon.config.oa.pending,
+			&ctx->perfmon.config.gp.pending,
+			dev_priv->perfmon.config.oa,
+			dev_priv->perfmon.config.gp,
+			false);
+
+	if (ret)
+		goto unlock_perfmon;
+
+	/*
+	 * return info about what is actualy set for submission in
+	 * target context
+	 */
+	args->gp_id = ctx->perfmon.config.gp.pending.id;
+	args->oa_id = ctx->perfmon.config.oa.pending.id;
+
+unlock_perfmon:
+	mutex_unlock(&dev_priv->perfmon.config.lock);
+unlock_dev:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+
+static void *emit_dword(void *mem, __u32 cmd)
+{
+	iowrite32(cmd, mem);
+	return ((__u32 *)mem) + 1;
+}
+
+static void *emit_load_register_imm(void *mem, __u32 reg, __u32 val)
+{
+	mem = emit_dword(mem, MI_NOOP);
+	mem = emit_dword(mem, MI_LOAD_REGISTER_IMM(1));
+	mem = emit_dword(mem, reg);
+	mem = emit_dword(mem, val);
+	return mem;
+}
+
+static void *emit_cs_stall_pipe_control(void *mem)
+{
+	mem = emit_dword(mem, GFX_OP_PIPE_CONTROL(6));
+	mem = emit_dword(mem, PIPE_CONTROL_CS_STALL|PIPE_CONTROL_WRITE_FLUSH|
+			      PIPE_CONTROL_GLOBAL_GTT_IVB);
+	mem = emit_dword(mem, 0);
+	mem = emit_dword(mem, 0);
+	mem = emit_dword(mem, 0);
+	mem = emit_dword(mem, 0);
+	return mem;
+}
+
+int i915_perfmon_update_workaround_bb(struct drm_i915_private *dev_priv,
+				      struct drm_i915_perfmon_config *config)
+{
+	const size_t commands_size = 6 + /* pipe control */
+				     config->size * 4 + /* NOOP + LRI */
+				     6 + /* pipe control */
+				     1;  /* BB end */
+	void *buffer_tail;
+	unsigned int i = 0;
+	int ret = 0;
+
+	if (commands_size > PAGE_SIZE) {
+		DRM_ERROR("OA cfg too long to fit into workarond BB\n");
+		return -ENOSPC;
+	}
+
+	BUG_ON(!mutex_is_locked(&dev_priv->perfmon.config.lock));
+
+	if (atomic_read(&dev_priv->perfmon.config.enable) == 0 ||
+	    !dev_priv->rc6_wa_bb.obj) {
+		DRM_ERROR("not ready to write WA BB commands\n");
+		return -EINVAL;
+	}
+
+	ret = mutex_lock_interruptible(&dev_priv->rc6_wa_bb.lock);
+	if (ret)
+		return ret;
+
+	if (!dev_priv->rc6_wa_bb.obj) {
+		mutex_unlock(&dev_priv->rc6_wa_bb.lock);
+		return 0;
+	}
+
+	intel_uncore_forcewake_get(dev_priv, FORCEWAKE_RENDER);
+
+	/* disable RC6 WA BB */
+	I915_WRITE(GEN8_RC6_WA_BB, 0x0);
+
+	buffer_tail = dev_priv->rc6_wa_bb.address;
+	buffer_tail = emit_cs_stall_pipe_control(buffer_tail);
+
+	/* OA/NOA config */
+	for (i = 0; i < config->size; i++)
+		buffer_tail = emit_load_register_imm(
+			buffer_tail,
+			config->entries[i].offset,
+			config->entries[i].value);
+
+	buffer_tail = emit_cs_stall_pipe_control(buffer_tail);
+
+	/* BB END */
+	buffer_tail = emit_dword(buffer_tail, MI_BATCH_BUFFER_END);
+
+	/* enable WA BB */
+	I915_WRITE(GEN8_RC6_WA_BB, dev_priv->rc6_wa_bb.offset | 0x1);
+
+	intel_uncore_forcewake_put(dev_priv, FORCEWAKE_RENDER);
+
+	mutex_unlock(&dev_priv->rc6_wa_bb.lock);
+
+	return 0;
+}
+
+static int allocate_wa_bb(struct drm_i915_private *dev_priv)
+{
+	int ret = 0;
+
+	BUG_ON(!mutex_is_locked(&dev_priv->dev->struct_mutex));
+
+	ret = mutex_lock_interruptible(&dev_priv->rc6_wa_bb.lock);
+	if (ret)
+		return ret;
+
+	if (atomic_inc_return(&dev_priv->rc6_wa_bb.enable) > 1) {
+		mutex_unlock(&dev_priv->rc6_wa_bb.lock);
+		return 0;
+	}
+
+	BUG_ON(dev_priv->rc6_wa_bb.obj != NULL);
+
+	dev_priv->rc6_wa_bb.obj = i915_gem_alloc_object(
+						dev_priv->dev,
+						PAGE_SIZE);
+	if (!dev_priv->rc6_wa_bb.obj) {
+		ret = -ENOMEM;
+		goto unlock;
+	}
+
+	ret = i915_gem_obj_ggtt_pin(
+			dev_priv->rc6_wa_bb.obj,
+			PAGE_SIZE, PIN_MAPPABLE);
+
+	if (ret) {
+		drm_gem_object_unreference_unlocked(
+			&dev_priv->rc6_wa_bb.obj->base);
+		goto unlock;
+	}
+
+	ret = i915_gem_object_set_to_gtt_domain(dev_priv->rc6_wa_bb.obj,
+						true);
+	if (ret) {
+		i915_gem_object_ggtt_unpin(dev_priv->rc6_wa_bb.obj);
+		drm_gem_object_unreference_unlocked(
+			&dev_priv->rc6_wa_bb.obj->base);
+		goto unlock;
+	}
+
+	dev_priv->rc6_wa_bb.offset = i915_gem_obj_ggtt_offset(
+						dev_priv->rc6_wa_bb.obj);
+
+	dev_priv->rc6_wa_bb.address = ioremap_wc(
+		dev_priv->ggtt.mappable_base + dev_priv->rc6_wa_bb.offset,
+		PAGE_SIZE);
+
+	if (!dev_priv->rc6_wa_bb.address) {
+		i915_gem_object_ggtt_unpin(dev_priv->rc6_wa_bb.obj);
+		drm_gem_object_unreference_unlocked(
+			&dev_priv->rc6_wa_bb.obj->base);
+		ret =  -ENOMEM;
+		goto unlock;
+	}
+
+	memset(dev_priv->rc6_wa_bb.address, 0, PAGE_SIZE);
+
+unlock:
+	if (ret) {
+		dev_priv->rc6_wa_bb.obj = NULL;
+		dev_priv->rc6_wa_bb.offset = 0;
+	}
+	mutex_unlock(&dev_priv->rc6_wa_bb.lock);
+
+	return ret;
+}
+
+static void deallocate_wa_bb(struct drm_i915_private *dev_priv)
+{
+	BUG_ON(!mutex_is_locked(&dev_priv->dev->struct_mutex));
+
+	mutex_lock(&dev_priv->rc6_wa_bb.lock);
+
+	if (atomic_read(&dev_priv->rc6_wa_bb.enable) == 0)
+		goto unlock;
+
+	if (atomic_dec_return(&dev_priv->rc6_wa_bb.enable) > 1)
+		goto unlock;
+
+	intel_uncore_forcewake_get(dev_priv, FORCEWAKE_RENDER);
+
+	I915_WRITE(GEN8_RC6_WA_BB, 0);
+
+	intel_uncore_forcewake_put(dev_priv, FORCEWAKE_RENDER);
+
+	if (dev_priv->rc6_wa_bb.obj != NULL) {
+		iounmap(dev_priv->rc6_wa_bb.address);
+		i915_gem_object_ggtt_unpin(dev_priv->rc6_wa_bb.obj);
+		drm_gem_object_unreference(&dev_priv->rc6_wa_bb.obj->base);
+		dev_priv->rc6_wa_bb.obj = NULL;
+		dev_priv->rc6_wa_bb.offset = 0;
+	}
+unlock:
+	mutex_unlock(&dev_priv->rc6_wa_bb.lock);
+}
+
+/**
+* i915_perfmon_config_enable_disable
+*
+* Enable/disable OA/GP configuration transport.
+*/
+static int i915_perfmon_config_enable_disable(
+	struct drm_device *dev,
+	int enable)
+{
+	int ret;
+
+	struct drm_i915_private *dev_priv =
+		(struct drm_i915_private *) dev->dev_private;
+
+	if (!(IS_GEN8(dev)) && !(IS_GEN9(dev)))
+		return -EINVAL;
+
+	ret = i915_mutex_lock_interruptible(dev_priv->dev);
+	if (ret)
+		return ret;
+
+	ret = mutex_lock_interruptible(&dev_priv->perfmon.config.lock);
+	if (ret) {
+		mutex_unlock(&dev->struct_mutex);
+		return ret;
+	}
+
+	if (enable) {
+		ret = allocate_wa_bb(dev_priv);
+		if (!ret &&
+		    atomic_inc_return(&dev_priv->perfmon.config.enable) == 1) {
+			dev_priv->perfmon.config.target =
+				I915_PERFMON_CONFIG_TARGET_ALL;
+			dev_priv->perfmon.config.oa.id = 0;
+			dev_priv->perfmon.config.gp.id = 0;
+		}
+	} else if (atomic_read(&dev_priv->perfmon.config.enable)) {
+		atomic_dec(&dev_priv->perfmon.config.enable);
+		deallocate_wa_bb(dev_priv);
+	}
+
+	mutex_unlock(&dev_priv->perfmon.config.lock);
+	mutex_unlock(&dev->struct_mutex);
+
+	return ret;
+}
+
+
+/**
+ * i915_perfmon_open
+ *
+ * open perfmon for current file
+ */
+static int i915_perfmon_open(
+	struct drm_file *file)
+{
+	struct drm_i915_file_private *file_priv = file->driver_priv;
+	int ret = 0;
+
+	if (!capable(CAP_SYS_ADMIN))
+		ret = -EACCES;
+	else
+		file_priv->perfmon.opened = true;
+
+	return ret;
+}
+
+/**
+ * i915_perfmon_close
+ *
+ * close perfmon for current file
+ */
+static int i915_perfmon_close(
+	struct drm_file *file)
+{
+	struct drm_i915_file_private *file_priv = file->driver_priv;
+
+	file_priv->perfmon.opened = false;
+
+	return 0;
+}
+
+
+int i915_perfmon_pin_oa_buffer(struct drm_device *dev,
+	struct drm_file *file,
+	struct drm_i915_perfmon_pin_oa_buffer* oa_buffer)
+{
+	int ret = 0;
+	struct drm_i915_gem_object *obj = NULL;
+
+	ret = i915_mutex_lock_interruptible(dev);
+	if (ret)
+		return ret;
+
+	obj = to_intel_bo(drm_gem_object_lookup(file, oa_buffer->handle));
+	if (&obj->base == NULL) {
+		ret = -ENOENT;
+		goto unlock;
+	}
+
+	ret = i915_gem_obj_ggtt_pin(obj, oa_buffer->alignment, PIN_MAPPABLE);
+
+	if (ret == 0)
+		oa_buffer->offset = i915_gem_obj_ggtt_offset(obj);
+
+	drm_gem_object_unreference(&obj->base);
+unlock:
+	mutex_unlock(&dev->struct_mutex);
+
+	return ret;
+}
+
+int i915_perfmon_unpin_oa_buffer(struct drm_device *dev,
+	struct drm_file *file,
+	struct drm_i915_perfmon_unpin_oa_buffer* oa_buffer)
+{
+	int ret = 0;
+	struct drm_i915_gem_object *obj = NULL;
+
+	ret = i915_mutex_lock_interruptible(dev);
+	if (ret)
+		return ret;
+
+	obj = to_intel_bo(drm_gem_object_lookup(file, oa_buffer->handle));
+	if (&obj->base == NULL) {
+		ret = -ENOENT;
+		goto unlock;
+	}
+
+	i915_gem_object_ggtt_unpin(obj);
+	drm_gem_object_unreference(&obj->base);
+unlock:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+
+/**
+ * i915_perfmon_ioctl - performance monitoring support
+ *
+ * Main entry point to performance monitoring support
+ * IOCTLs.
+ */
+int i915_perfmon_ioctl(struct drm_device *dev, void *data,
+	struct drm_file *file)
+{
+	struct drm_i915_perfmon *perfmon = data;
+	struct drm_i915_file_private *file_priv = file->driver_priv;
+	int ret = 0;
+	switch (perfmon->op) {
+	case I915_PERFMON_OPEN:
+		ret = i915_perfmon_open(file);
+		break;
+	case I915_PERFMON_CLOSE:
+		ret = i915_perfmon_close(file);
+		break;
+	case I915_PERFMON_ENABLE_CONFIG:
+		ret = i915_perfmon_config_enable_disable(dev, 1);
+		break;
+	case I915_PERFMON_DISABLE_CONFIG:
+		ret = i915_perfmon_config_enable_disable(dev, 0);
+		break;
+	case I915_PERFMON_SET_CONFIG:
+		if (!file_priv->perfmon.opened) {
+			ret = -EACCES;
+			break;
+		}
+		ret = i915_perfmon_set_config(
+			dev,
+			&perfmon->data.set_config);
+		break;
+	case I915_PERFMON_LOAD_CONFIG:
+		ret = i915_perfmon_load_config(
+			dev,
+			file,
+			&perfmon->data.load_config);
+		break;
+	case I915_PERFMON_GET_HW_CTX_ID:
+		ret = i915_perfmon_get_hw_ctx_id(
+			dev,
+			file,
+			&perfmon->data.get_hw_ctx_id);
+		break;
+	case I915_PERFMON_GET_HW_CTX_IDS:
+		if (!file_priv->perfmon.opened) {
+			ret = -EACCES;
+			break;
+		}
+		ret = i915_perfmon_get_hw_ctx_ids(
+			dev,
+			&perfmon->data.get_hw_ctx_ids);
+		break;
+	case I915_PERFMON_PIN_OA_BUFFER:
+		ret = i915_perfmon_pin_oa_buffer(
+			dev,
+			file,
+			&perfmon->data.pin_oa_buffer);
+		break;
+	case I915_PERFMON_UNPIN_OA_BUFFER:
+		ret = i915_perfmon_unpin_oa_buffer(
+			dev,
+			file,
+			&perfmon->data.unpin_oa_buffer);
+		break;
+	default:
+		DRM_DEBUG("UNKNOWN OP\n");
+		/* unknown operation */
+		ret = -EINVAL;
+		break;
+	}
+
+	return ret;
+}
+
+void i915_perfmon_setup(struct drm_i915_private *dev_priv)
+{
+	atomic_set(&dev_priv->perfmon.config.enable, 0);
+	dev_priv->perfmon.config.oa.entries = NULL;
+	dev_priv->perfmon.config.gp.entries = NULL;
+}
+
+void i915_perfmon_cleanup(struct drm_i915_private *dev_priv)
+{
+	kfree(dev_priv->perfmon.config.oa.entries);
+	kfree(dev_priv->perfmon.config.gp.entries);
+}
+
+void i915_perfmon_ctx_setup(struct intel_context *ctx)
+{
+	ctx->perfmon.config.oa.pending.entries = NULL;
+	ctx->perfmon.config.gp.pending.entries = NULL;
+}
+
+void i915_perfmon_ctx_cleanup(struct intel_context *ctx)
+{
+	kfree(ctx->perfmon.config.oa.pending.entries);
+	kfree(ctx->perfmon.config.gp.pending.entries);
+}
+
+/////////////////////////////////////////////////////////////////
+void perfmon_send_config(
+	struct intel_ringbuffer *ringbuf,
+	struct drm_i915_perfmon_config *config)
+{
+	int i;
+
+	for (i = 0; i < config->size; i++) {
+		DRM_DEBUG("perfmon config %x reg:%05x val:%08x\n",
+			config->id,
+			config->entries[i].offset,
+			config->entries[i].value);
+		intel_logical_ring_emit(ringbuf, MI_NOOP);
+		intel_logical_ring_emit(ringbuf, MI_LOAD_REGISTER_IMM(1));
+		intel_logical_ring_emit(ringbuf, config->entries[i].offset);
+		intel_logical_ring_emit(ringbuf, config->entries[i].value);
+        }
+
+}
+
+static inline struct drm_i915_perfmon_config *get_perfmon_config(
+	struct drm_i915_private *dev_priv,
+	struct intel_context *ctx,
+	struct drm_i915_perfmon_config *config_global,
+	struct drm_i915_perfmon_config *config_context,
+	__u32 ctx_submitted_config_id)
+{
+        struct drm_i915_perfmon_config *config  = NULL;
+        enum DRM_I915_PERFMON_CONFIG_TARGET target;
+
+        BUG_ON(!mutex_is_locked(&dev_priv->perfmon.config.lock));
+
+        target = dev_priv->perfmon.config.target;
+        switch (target) {
+        case I915_PERFMON_CONFIG_TARGET_CTX:
+                config = config_context;
+                break;
+        case I915_PERFMON_CONFIG_TARGET_PID:
+                if (pid_vnr(ctx->pid) == dev_priv->perfmon.config.pid)
+                        config = config_global;
+                break;
+        case I915_PERFMON_CONFIG_TARGET_ALL:
+                config = config_global;
+                break;
+        default:
+                BUG_ON(1);
+                break;
+        }
+
+        if (config != NULL) {
+                if (config->size == 0 || config->id == 0) {
+                        /* configuration is empty or targets other context */
+                        DRM_DEBUG("perfmon configuration empty\n");
+                        config = NULL;
+                } else if (config->id == ctx_submitted_config_id) {
+                        /* configuration is already submitted in this context*/
+                        DRM_DEBUG("perfmon configuration %x is submitted\n",
+                                        config->id);
+                        config = NULL;
+                }
+        }
+
+        if (config != NULL)
+                DRM_DEBUG("perfmon configuration TARGET:%u SIZE:%x ID:%x",
+                        target,
+                        config->size,
+                        config->id);
+
+        return config;
+}
+
+int
+i915_program_perfmon(struct drm_device *dev,
+		     struct drm_i915_gem_request *req)
+{
+        struct drm_i915_private *dev_priv = dev->dev_private;
+        struct drm_i915_perfmon_config *config_oa, *config_gp;
+        struct intel_ringbuffer *ringbuf = req->ringbuf;
+        struct intel_context *ctx = req->ctx;
+        size_t size;
+        int ret = 0;
+
+        if (!atomic_read(&dev_priv->perfmon.config.enable) &&
+            ctx->perfmon.config.oa.submitted_id == 0)
+                return 0;
+
+        ret = mutex_lock_interruptible(&dev_priv->perfmon.config.lock);
+
+        if (ret)
+                return ret;
+
+
+        if (!atomic_read(&dev_priv->perfmon.config.enable)) {
+                if (ctx->perfmon.config.oa.submitted_id != 0) {
+                        /* write 0 to OA_CTX_CONTROL to stop counters */
+                        ret = intel_ring_begin(req, 4);
+                        if (!ret) {
+                                intel_logical_ring_emit(ringbuf, MI_NOOP);
+                                intel_logical_ring_emit(ringbuf,
+                                        MI_LOAD_REGISTER_IMM(1));
+                                intel_logical_ring_emit_reg(ringbuf,
+                                        GEN8_OA_CTX_CONTROL);
+                                intel_logical_ring_emit(ringbuf, 0);
+                                intel_logical_ring_advance(ringbuf);
+                        }
+                        ctx->perfmon.config.oa.submitted_id = 0;
+                }
+                goto unlock;
+        }
+
+        /* check for pending OA config */
+        config_oa = get_perfmon_config(dev_priv, ctx,
+                                        &dev_priv->perfmon.config.oa,
+                                        &ctx->perfmon.config.oa.pending,
+                                        ctx->perfmon.config.oa.submitted_id);
+
+        /* check for pending PERFMON config */
+        config_gp = get_perfmon_config(dev_priv, ctx,
+                                        &dev_priv->perfmon.config.gp,
+                                        &ctx->perfmon.config.gp.pending,
+                                        ctx->perfmon.config.gp.submitted_id);
+
+        size = (config_oa ? config_oa->size : 0) +
+               (config_gp ? config_gp->size : 0);
+
+        if (size == 0)
+                goto unlock;
+
+        ret = intel_ring_begin(req, 4 * size);
+        if (ret)
+                goto unlock;
+
+        /* submit pending OA config */
+        if (config_oa) {
+                perfmon_send_config(
+                        ringbuf,
+                        config_oa);
+                ctx->perfmon.config.oa.submitted_id = config_oa->id;
+
+                i915_perfmon_update_workaround_bb(dev_priv, config_oa);
+        }
+
+        /* submit pending general purpose perfmon counters config */
+        if (config_gp) {
+                perfmon_send_config(
+                        ringbuf,
+                        config_gp);
+                ctx->perfmon.config.gp.submitted_id = config_gp->id;
+        }
+        intel_logical_ring_advance(ringbuf);
+unlock:
+        mutex_unlock(&dev_priv->perfmon.config.lock);
+        return ret;
+}
+
+
+
diff --git a/drivers/gpu/drm/i915/i915_perfmon_defs.h b/drivers/gpu/drm/i915/i915_perfmon_defs.h
new file mode 100644
index 0000000..300f3e5
--- /dev/null
+++ b/drivers/gpu/drm/i915/i915_perfmon_defs.h
@@ -0,0 +1,62 @@
+/*
+ * Copyright  2014 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ */
+
+#ifndef _I915_PERFMON_DEFS_H_
+#define _I915_PERFMON_DEFS_H_
+
+struct drm_i915_perfmon_config {
+	struct drm_i915_perfmon_config_entry *entries;
+	__u32 size;
+	__u32  id;
+};
+
+struct drm_i915_perfmon_context {
+	struct {
+		struct {
+			struct drm_i915_perfmon_config pending;
+			__u32 submitted_id;
+		} oa, gp;
+	} config;
+};
+
+struct drm_i915_perfmon_device {
+	/* perfmon interrupt support */
+	wait_queue_head_t	buffer_queue;
+	atomic_t		buffer_interrupts;
+
+	/* perfmon counters configuration */
+	struct {
+		struct drm_i915_perfmon_config oa;
+		struct drm_i915_perfmon_config gp;
+		enum DRM_I915_PERFMON_CONFIG_TARGET target;
+		pid_t pid;
+		atomic_t enable;
+		struct mutex lock;
+	} config;
+};
+
+struct drm_i915_perfmon_file {
+	bool opened;
+};
+
+#endif	/* _I915_PERFMON_DEFS_H_ */
diff --git a/drivers/gpu/drm/i915/i915_reg.h b/drivers/gpu/drm/i915/i915_reg.h
index 3fcf7dd..14e4b54 100644
--- a/drivers/gpu/drm/i915/i915_reg.h
+++ b/drivers/gpu/drm/i915/i915_reg.h
@@ -8356,4 +8356,7 @@ enum skl_disp_power_wells {
 #define   GEN9_L3_LRA_1_GPGPU_DEFAULT_VALUE_SKL  0x67F1427F /*    "        " */
 #define   GEN9_L3_LRA_1_GPGPU_DEFAULT_VALUE_BXT  0x5FF101FF /*    "        " */
 
+#define GEN8_OA_CTX_CONTROL _MMIO(0x2360)
+#define GEN8_RC6_WA_BB      _MMIO(0x2058)
+
 #endif /* _I915_REG_H_ */
diff --git a/drivers/gpu/drm/i915/intel_lrc.c b/drivers/gpu/drm/i915/intel_lrc.c
index bb49260..42123cb 100644
--- a/drivers/gpu/drm/i915/intel_lrc.c
+++ b/drivers/gpu/drm/i915/intel_lrc.c
@@ -227,6 +227,10 @@ enum {
 #define GEN8_CTX_RCS_INDIRECT_CTX_OFFSET_DEFAULT	0x17
 #define GEN9_CTX_RCS_INDIRECT_CTX_OFFSET_DEFAULT	0x26
 
+extern int i915_program_perfmon(struct drm_device *dev,
+				struct drm_i915_gem_request *req
+);
+
 static int intel_lr_context_pin(struct intel_context *ctx,
 				struct intel_engine_cs *engine);
 
@@ -901,6 +905,9 @@ int intel_execlists_submission(struct i915_execbuffer_params *params,
 	if (ret)
 		return ret;
 
+	if ((IS_GEN8(dev) || IS_GEN9(dev)) && engine == &dev_priv->engine[RCS])
+		i915_program_perfmon(dev, params->request);
+
 	exec_start = params->batch_obj_vm_offset +
 		     args->batch_start_offset;
 
diff --git a/include/uapi/drm/i915_drm.h b/include/uapi/drm/i915_drm.h
index 2843d5e..20631d0 100644
--- a/include/uapi/drm/i915_drm.h
+++ b/include/uapi/drm/i915_drm.h
@@ -28,6 +28,7 @@
 #define _UAPI_I915_DRM_H_
 
 #include "drm.h"
+#include "i915_perfmon.h"
 
 #if defined(__cplusplus)
 extern "C" {
@@ -234,6 +235,7 @@ typedef struct _drm_i915_sarea {
 #define DRM_I915_GEM_USERPTR		0x33
 #define DRM_I915_GEM_CONTEXT_GETPARAM	0x34
 #define DRM_I915_GEM_CONTEXT_SETPARAM	0x35
+#define DRM_I915_PERFMON		0x3e
 
 #define DRM_IOCTL_I915_INIT		DRM_IOW( DRM_COMMAND_BASE + DRM_I915_INIT, drm_i915_init_t)
 #define DRM_IOCTL_I915_FLUSH		DRM_IO ( DRM_COMMAND_BASE + DRM_I915_FLUSH)
@@ -288,6 +290,8 @@ typedef struct _drm_i915_sarea {
 #define DRM_IOCTL_I915_GEM_CONTEXT_GETPARAM	DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_GEM_CONTEXT_GETPARAM, struct drm_i915_gem_context_param)
 #define DRM_IOCTL_I915_GEM_CONTEXT_SETPARAM	DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_GEM_CONTEXT_SETPARAM, struct drm_i915_gem_context_param)
 
+#define DRM_IOCTL_I915_PERFMON		DRM_IOWR(DRM_COMMAND_BASE + DRM_I915_PERFMON, struct drm_i915_perfmon)
+
 /* Allow drivers to submit batchbuffers directly to hardware, relying
  * on the security mechanisms provided by hardware.
  */
diff --git a/include/uapi/drm/i915_perfmon.h b/include/uapi/drm/i915_perfmon.h
new file mode 100644
index 0000000..ebc7498
--- /dev/null
+++ b/include/uapi/drm/i915_perfmon.h
@@ -0,0 +1,125 @@
+/*
+ * Copyright  2013 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ */
+
+#ifndef _I915_PERFMON_H_
+#define _I915_PERFMON_H_
+
+#define I915_PERFMON_IOCTL_VERSION      5
+
+struct drm_i915_perfmon_config_entry {
+	__u32 offset;
+	__u32 value;
+};
+
+static const unsigned int I915_PERFMON_CONFIG_SIZE = 256;
+
+/* Explicitly aligned to 8 bytes to avoid mismatch
+   between 64-bit KM and 32-bit UM. */
+typedef __u64 drm_i915_perfmon_shared_ptr __aligned(8);
+
+struct drm_i915_perfmon_user_config {
+	/* This is pointer to struct drm_i915_perfmon_config_entry.*/
+	drm_i915_perfmon_shared_ptr entries;
+	__u32 size;
+	__u32 id;
+};
+
+enum DRM_I915_PERFMON_CONFIG_TARGET {
+	I915_PERFMON_CONFIG_TARGET_CTX,
+	I915_PERFMON_CONFIG_TARGET_PID,
+	I915_PERFMON_CONFIG_TARGET_ALL,
+};
+
+struct drm_i915_perfmon_set_config {
+	enum DRM_I915_PERFMON_CONFIG_TARGET target;
+	struct drm_i915_perfmon_user_config oa;
+	struct drm_i915_perfmon_user_config gp;
+	__u32 pid;
+};
+
+struct drm_i915_perfmon_load_config {
+	__u32 ctx_id;
+	__u32 oa_id;
+	__u32 gp_id;
+};
+
+
+static const unsigned int I915_PERFMON_MAX_HW_CTX_IDS = 1024;
+
+struct drm_i915_perfmon_get_hw_ctx_ids {
+	__u32 pid;
+	__u32 count;
+	 /* This is pointer to __u32. */
+	drm_i915_perfmon_shared_ptr ids;
+};
+
+
+struct drm_i915_perfmon_get_hw_ctx_id {
+	__u32 ctx_id;
+	__u32 hw_ctx_id;
+};
+
+struct drm_i915_perfmon_pin_oa_buffer {
+	/** Handle of the buffer to be pinned. */
+        __u32 handle;
+        __u32 pad;
+
+        /** alignment required within the aperture */
+        __u64 alignment;
+
+        /** Returned GTT offset of the buffer. */
+        __u64 offset;
+};
+
+struct drm_i915_perfmon_unpin_oa_buffer {
+	/** Handle of the buffer to be pinned. */
+        __u32 handle;
+        __u32 pad;
+};
+
+enum I915_PERFMON_IOCTL_OP {
+	I915_PERFMON_OPEN = 8,
+	I915_PERFMON_CLOSE,
+	I915_PERFMON_ENABLE_CONFIG,
+	I915_PERFMON_DISABLE_CONFIG,
+	I915_PERFMON_SET_CONFIG,
+	I915_PERFMON_LOAD_CONFIG,
+	I915_PERFMON_GET_HW_CTX_ID,
+	I915_PERFMON_GET_HW_CTX_IDS,
+	I915_PERFMON_PIN_OA_BUFFER,
+	I915_PERFMON_UNPIN_OA_BUFFER,
+};
+
+struct drm_i915_perfmon {
+	enum I915_PERFMON_IOCTL_OP op;
+	union {
+		struct drm_i915_perfmon_set_config	set_config;
+		struct drm_i915_perfmon_load_config	load_config;
+		struct drm_i915_perfmon_get_hw_ctx_id	get_hw_ctx_id;
+		struct drm_i915_perfmon_get_hw_ctx_ids	get_hw_ctx_ids;
+		struct drm_i915_perfmon_pin_oa_buffer   pin_oa_buffer;
+		struct drm_i915_perfmon_unpin_oa_buffer	unpin_oa_buffer;
+	} data;
+};
+
+#endif	/* _I915_PERFMON_H_ */
-- 
1.7.1


From 029d7d32ef4595769ae6013defbf764caf239206 Mon Sep 17 00:00:00 2001
From: Woo, Insoo <insoo.woo@intel.com>
Date: Wed, 2 Mar 2016 10:23:03 -0800
Subject: [PATCH 06/17] drm/i915 - Allow a render node to use perfmon ioctl

OCL PerfCounter Extension needs perfmon ioctl to send its context
OA reg configuration for per-context perf monitoring.
For this purpose, this patch allows a render node to access the ioctl
interfaces.

Change-Id: Idd6b6a850ed3c4f515044471fb8b3a4d95efbc8d
Signed-off-by: Woo, Insoo <insoo.woo@intel.com>
(cherry picked from commit c4a4acd0261b8128f5d4a98296643662dac441dc)
---
 drivers/gpu/drm/i915/i915_dma.c |    2 +-
 1 files changed, 1 insertions(+), 1 deletions(-)

diff --git a/drivers/gpu/drm/i915/i915_dma.c b/drivers/gpu/drm/i915/i915_dma.c
index 5be83cc..dd8ea26 100644
--- a/drivers/gpu/drm/i915/i915_dma.c
+++ b/drivers/gpu/drm/i915/i915_dma.c
@@ -1596,7 +1596,7 @@ const struct drm_ioctl_desc i915_ioctls[] = {
 	DRM_IOCTL_DEF_DRV(I915_GEM_USERPTR, i915_gem_userptr_ioctl, DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(I915_GEM_CONTEXT_GETPARAM, i915_gem_context_getparam_ioctl, DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(I915_GEM_CONTEXT_SETPARAM, i915_gem_context_setparam_ioctl, DRM_RENDER_ALLOW),
-	DRM_IOCTL_DEF_DRV(I915_PERFMON, i915_perfmon_ioctl, DRM_UNLOCKED),
+	DRM_IOCTL_DEF_DRV(I915_PERFMON, i915_perfmon_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
 };
 
 int i915_max_ioctl = ARRAY_SIZE(i915_ioctls);
-- 
1.7.1


From dae87ed4ed8fcb5066444961ab18737f4b29881c Mon Sep 17 00:00:00 2001
From: Andrzej Datczuk <andrzej.datczuk@intel.com>
Date: Fri, 20 May 2016 12:31:43 +0200
Subject: [PATCH 07/17] drm/i915: Fix ref counting for RC6 WA BB alloc/dealloc

This patch fixes incorrect states of RC6 WA BB allocation reference
counter which resulted in RC6 WA BB deallocation when there was stil
one client attached.

This change should eliminate error message 'RC6 WA BB not ready'
showing in logs from i915 kernel.

Signed-off-by: Andrzej Datczuk <andrzej.datczuk@intel.com>
(cherry picked from commit 12e926859eda5dd76df4f25f05f8f81d13cadb7f)
---
 drivers/gpu/drm/i915/i915_perfmon.c |    3 ++-
 1 files changed, 2 insertions(+), 1 deletions(-)

diff --git a/drivers/gpu/drm/i915/i915_perfmon.c b/drivers/gpu/drm/i915/i915_perfmon.c
index fb3df80..bde7bfb 100644
--- a/drivers/gpu/drm/i915/i915_perfmon.c
+++ b/drivers/gpu/drm/i915/i915_perfmon.c
@@ -633,6 +633,7 @@ static int allocate_wa_bb(struct drm_i915_private *dev_priv)
 
 unlock:
 	if (ret) {
+		atomic_dec_return(&dev_priv->rc6_wa_bb.enable);
 		dev_priv->rc6_wa_bb.obj = NULL;
 		dev_priv->rc6_wa_bb.offset = 0;
 	}
@@ -650,7 +651,7 @@ static void deallocate_wa_bb(struct drm_i915_private *dev_priv)
 	if (atomic_read(&dev_priv->rc6_wa_bb.enable) == 0)
 		goto unlock;
 
-	if (atomic_dec_return(&dev_priv->rc6_wa_bb.enable) > 1)
+	if (atomic_dec_return(&dev_priv->rc6_wa_bb.enable) > 0)
 		goto unlock;
 
 	intel_uncore_forcewake_get(dev_priv, FORCEWAKE_RENDER);
-- 
1.7.1


From d4b467340ab1cc4ad13b219d123becc8afac12e1 Mon Sep 17 00:00:00 2001
From: Rafal Sapala <rafal.a.sapala@intel.com>
Date: Fri, 3 Jun 2016 13:19:31 +0200
Subject: [PATCH 08/17] drm/i915/perfmon: Fixup locking on error path in RC6 WA BB alloc

Change-Id: Ib4e6196f3052215ee52d2f9f5bc7ca7ee5d2d0a9
Signed-off-by: Rafal Sapala <rafal.a.sapala@intel.com>
(cherry picked from commit 2e78097d226959cacb2c30f22c9c0d80eeabe214)
---
 drivers/gpu/drm/i915/i915_perfmon.c |   11 ++++-------
 1 files changed, 4 insertions(+), 7 deletions(-)

diff --git a/drivers/gpu/drm/i915/i915_perfmon.c b/drivers/gpu/drm/i915/i915_perfmon.c
index bde7bfb..eddb2fa 100644
--- a/drivers/gpu/drm/i915/i915_perfmon.c
+++ b/drivers/gpu/drm/i915/i915_perfmon.c
@@ -600,8 +600,7 @@ static int allocate_wa_bb(struct drm_i915_private *dev_priv)
 			PAGE_SIZE, PIN_MAPPABLE);
 
 	if (ret) {
-		drm_gem_object_unreference_unlocked(
-			&dev_priv->rc6_wa_bb.obj->base);
+		drm_gem_object_unreference(&dev_priv->rc6_wa_bb.obj->base);
 		goto unlock;
 	}
 
@@ -609,8 +608,7 @@ static int allocate_wa_bb(struct drm_i915_private *dev_priv)
 						true);
 	if (ret) {
 		i915_gem_object_ggtt_unpin(dev_priv->rc6_wa_bb.obj);
-		drm_gem_object_unreference_unlocked(
-			&dev_priv->rc6_wa_bb.obj->base);
+		drm_gem_object_unreference(&dev_priv->rc6_wa_bb.obj->base);
 		goto unlock;
 	}
 
@@ -623,8 +621,7 @@ static int allocate_wa_bb(struct drm_i915_private *dev_priv)
 
 	if (!dev_priv->rc6_wa_bb.address) {
 		i915_gem_object_ggtt_unpin(dev_priv->rc6_wa_bb.obj);
-		drm_gem_object_unreference_unlocked(
-			&dev_priv->rc6_wa_bb.obj->base);
+		drm_gem_object_unreference(&dev_priv->rc6_wa_bb.obj->base);
 		ret =  -ENOMEM;
 		goto unlock;
 	}
@@ -633,7 +630,7 @@ static int allocate_wa_bb(struct drm_i915_private *dev_priv)
 
 unlock:
 	if (ret) {
-		atomic_dec_return(&dev_priv->rc6_wa_bb.enable);
+		atomic_dec(&dev_priv->rc6_wa_bb.enable);
 		dev_priv->rc6_wa_bb.obj = NULL;
 		dev_priv->rc6_wa_bb.offset = 0;
 	}
-- 
1.7.1


From 84a2dcd0f5331e46532d84ab089af21423894526 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Micha=C5=82=20Winiarski?= <michal.winiarski@intel.com>
Date: Thu, 13 Oct 2016 14:02:40 +0200
Subject: [PATCH 09/17] drm/i915: Remove unused "valid" parameter from pte_encode
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

We never used any invalid ptes, those were put in place for
a possibility of doing gpu faults. However our batchbuffers are not
restricted in length, so everything needs to be pointing to something
and thus out-of-bounds is pointing to scratch.

Remove the valid flag as it is always true.

v2: Expand commit msg, patch reorder (Mika)

Cc: Chris Wilson <chris@chris-wilson.co.uk>
Cc: Michel Thierry <michel.thierry@intel.com>
Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
Reviewed-by: Mika Kuoppala <mika.kuoppala@intel.com>
Signed-off-by: Michał Winiarski <michal.winiarski@intel.com>
Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
Link: http://patchwork.freedesktop.org/patch/msgid/1476360162-24062-1-git-send-email-michal.winiarski@intel.com

Conflicts:
	drivers/gpu/drm/i915/i915_gem.c
	drivers/gpu/drm/i915/i915_gem_execbuffer.c
	drivers/gpu/drm/i915/i915_gem_gtt.c
	drivers/gpu/drm/i915/i915_gem_gtt.h
	drivers/gpu/drm/i915/i915_gpu_error.c
---
 drivers/gpu/drm/i915/i915_gem_gtt.c |   94 +++++++++++++++--------------------
 drivers/gpu/drm/i915/i915_gem_gtt.h |    5 +-
 2 files changed, 42 insertions(+), 57 deletions(-)

diff --git a/drivers/gpu/drm/i915/i915_gem_gtt.c b/drivers/gpu/drm/i915/i915_gem_gtt.c
index 92acdff..3a121bc 100644
--- a/drivers/gpu/drm/i915/i915_gem_gtt.c
+++ b/drivers/gpu/drm/i915/i915_gem_gtt.c
@@ -173,15 +173,13 @@ static void ppgtt_unbind_vma(struct i915_vma *vma)
 {
 	vma->vm->clear_range(vma->vm,
 			     vma->node.start,
-			     vma->obj->base.size,
-			     true);
+			     vma->obj->base.size);
 }
 
 static gen8_pte_t gen8_pte_encode(dma_addr_t addr,
-				  enum i915_cache_level level,
-				  bool valid)
+				  enum i915_cache_level level)
 {
-	gen8_pte_t pte = valid ? _PAGE_PRESENT | _PAGE_RW : 0;
+	gen8_pte_t pte = _PAGE_PRESENT | _PAGE_RW;
 	pte |= addr;
 
 	switch (level) {
@@ -216,9 +214,9 @@ static gen8_pde_t gen8_pde_encode(const dma_addr_t addr,
 
 static gen6_pte_t snb_pte_encode(dma_addr_t addr,
 				 enum i915_cache_level level,
-				 bool valid, u32 unused)
+				 u32 unused)
 {
-	gen6_pte_t pte = valid ? GEN6_PTE_VALID : 0;
+	gen6_pte_t pte = GEN6_PTE_VALID;
 	pte |= GEN6_PTE_ADDR_ENCODE(addr);
 
 	switch (level) {
@@ -238,9 +236,9 @@ static gen6_pte_t snb_pte_encode(dma_addr_t addr,
 
 static gen6_pte_t ivb_pte_encode(dma_addr_t addr,
 				 enum i915_cache_level level,
-				 bool valid, u32 unused)
+				 u32 unused)
 {
-	gen6_pte_t pte = valid ? GEN6_PTE_VALID : 0;
+	gen6_pte_t pte = GEN6_PTE_VALID;
 	pte |= GEN6_PTE_ADDR_ENCODE(addr);
 
 	switch (level) {
@@ -262,9 +260,9 @@ static gen6_pte_t ivb_pte_encode(dma_addr_t addr,
 
 static gen6_pte_t byt_pte_encode(dma_addr_t addr,
 				 enum i915_cache_level level,
-				 bool valid, u32 flags)
+				 u32 flags)
 {
-	gen6_pte_t pte = valid ? GEN6_PTE_VALID : 0;
+	gen6_pte_t pte = GEN6_PTE_VALID;
 	pte |= GEN6_PTE_ADDR_ENCODE(addr);
 
 	if (!(flags & PTE_READ_ONLY))
@@ -278,9 +276,9 @@ static gen6_pte_t byt_pte_encode(dma_addr_t addr,
 
 static gen6_pte_t hsw_pte_encode(dma_addr_t addr,
 				 enum i915_cache_level level,
-				 bool valid, u32 unused)
+				 u32 unused)
 {
-	gen6_pte_t pte = valid ? GEN6_PTE_VALID : 0;
+	gen6_pte_t pte = GEN6_PTE_VALID;
 	pte |= HSW_PTE_ADDR_ENCODE(addr);
 
 	if (level != I915_CACHE_NONE)
@@ -291,9 +289,9 @@ static gen6_pte_t hsw_pte_encode(dma_addr_t addr,
 
 static gen6_pte_t iris_pte_encode(dma_addr_t addr,
 				  enum i915_cache_level level,
-				  bool valid, u32 unused)
+				  u32 unused)
 {
-	gen6_pte_t pte = valid ? GEN6_PTE_VALID : 0;
+	gen6_pte_t pte = GEN6_PTE_VALID;
 	pte |= HSW_PTE_ADDR_ENCODE(addr);
 
 	switch (level) {
@@ -467,7 +465,7 @@ static void gen8_initialize_pt(struct i915_address_space *vm,
 	gen8_pte_t scratch_pte;
 
 	scratch_pte = gen8_pte_encode(px_dma(vm->scratch_page),
-				      I915_CACHE_LLC, true);
+				      I915_CACHE_LLC);
 
 	fill_px(vm->dev, pt, scratch_pte);
 }
@@ -480,7 +478,7 @@ static void gen6_initialize_pt(struct i915_address_space *vm,
 	WARN_ON(px_dma(vm->scratch_page) == 0);
 
 	scratch_pte = vm->pte_encode(px_dma(vm->scratch_page),
-				     I915_CACHE_LLC, true, 0);
+				     I915_CACHE_LLC, 0);
 
 	fill32_px(vm->dev, pt, scratch_pte);
 }
@@ -757,13 +755,11 @@ static void gen8_ppgtt_clear_pte_range(struct i915_address_space *vm,
 }
 
 static void gen8_ppgtt_clear_range(struct i915_address_space *vm,
-				   uint64_t start,
-				   uint64_t length,
-				   bool use_scratch)
+				   uint64_t start, uint64_t length)
 {
 	struct i915_hw_ppgtt *ppgtt = i915_vm_to_ppgtt(vm);
 	gen8_pte_t scratch_pte = gen8_pte_encode(px_dma(vm->scratch_page),
-						 I915_CACHE_LLC, use_scratch);
+						 I915_CACHE_LLC);
 
 	if (!USES_FULL_48BIT_PPGTT(vm->dev)) {
 		gen8_ppgtt_clear_pte_range(vm, &ppgtt->pdp, start, length,
@@ -803,7 +799,7 @@ gen8_ppgtt_insert_pte_entries(struct i915_address_space *vm,
 
 		pt_vaddr[pte] =
 			gen8_pte_encode(sg_page_iter_dma_address(sg_iter),
-					cache_level, true);
+					cache_level);
 		if (++pte == GEN8_PTES) {
 			kunmap_px(ppgtt, pt_vaddr);
 			pt_vaddr = NULL;
@@ -1438,8 +1434,9 @@ static void gen8_dump_ppgtt(struct i915_hw_ppgtt *ppgtt, struct seq_file *m)
 	struct i915_address_space *vm = &ppgtt->base;
 	uint64_t start = ppgtt->base.start;
 	uint64_t length = ppgtt->base.total;
+
 	gen8_pte_t scratch_pte = gen8_pte_encode(px_dma(vm->scratch_page),
-						 I915_CACHE_LLC, true);
+						 I915_CACHE_LLC);
 
 	if (!USES_FULL_48BIT_PPGTT(vm->dev)) {
 		gen8_dump_pdp(&ppgtt->pdp, start, length, scratch_pte, m);
@@ -1556,7 +1553,7 @@ static void gen6_dump_ppgtt(struct i915_hw_ppgtt *ppgtt, struct seq_file *m)
 	uint32_t start = ppgtt->base.start, length = ppgtt->base.total;
 
 	scratch_pte = vm->pte_encode(px_dma(vm->scratch_page),
-				     I915_CACHE_LLC, true, 0);
+				     I915_CACHE_LLC, 0);
 
 	gen6_for_each_pde(unused, &ppgtt->pd, start, length, temp, pde) {
 		u32 expected;
@@ -1783,8 +1780,7 @@ static void gen6_ppgtt_enable(struct drm_device *dev)
 /* PPGTT support for Sandybdrige/Gen6 and later */
 static void gen6_ppgtt_clear_range(struct i915_address_space *vm,
 				   uint64_t start,
-				   uint64_t length,
-				   bool use_scratch)
+				   uint64_t length)
 {
 	struct i915_hw_ppgtt *ppgtt = i915_vm_to_ppgtt(vm);
 	gen6_pte_t *pt_vaddr, scratch_pte;
@@ -1795,7 +1791,7 @@ static void gen6_ppgtt_clear_range(struct i915_address_space *vm,
 	unsigned last_pte, i;
 
 	scratch_pte = vm->pte_encode(px_dma(vm->scratch_page),
-				     I915_CACHE_LLC, true, 0);
+				     I915_CACHE_LLC, 0);
 
 	while (num_entries) {
 		last_pte = first_pte + num_entries;
@@ -1834,7 +1830,7 @@ static void gen6_ppgtt_insert_entries(struct i915_address_space *vm,
 
 		pt_vaddr[act_pte] =
 			vm->pte_encode(sg_page_iter_dma_address(&sg_iter),
-				       cache_level, true, flags);
+				       cache_level, flags);
 
 		if (++act_pte == GEN6_PTES) {
 			kunmap_px(ppgtt, pt_vaddr);
@@ -2326,8 +2322,7 @@ void i915_gem_suspend_gtt_mappings(struct drm_device *dev)
 
 	i915_check_and_clear_faults(dev);
 
-	ggtt->base.clear_range(&ggtt->base, ggtt->base.start, ggtt->base.total,
-			     true);
+	ggtt->base.clear_range(&ggtt->base, ggtt->base.start, ggtt->base.total);
 
 	i915_ggtt_flush(dev_priv);
 }
@@ -2373,7 +2368,7 @@ static void gen8_ggtt_insert_entries(struct i915_address_space *vm,
 		addr = sg_dma_address(sg_iter.sg) +
 			(sg_iter.sg_pgoffset << PAGE_SHIFT);
 		gen8_set_pte(&gtt_entries[i],
-			     gen8_pte_encode(addr, level, true));
+			     gen8_pte_encode(addr, level));
 		i++;
 	}
 
@@ -2386,7 +2381,7 @@ static void gen8_ggtt_insert_entries(struct i915_address_space *vm,
 	 */
 	if (i != 0)
 		WARN_ON(readq(&gtt_entries[i-1])
-			!= gen8_pte_encode(addr, level, true));
+			!= gen8_pte_encode(addr, level));
 
 	/* This next bit makes the above posting read even more important. We
 	 * want to flush the TLBs only after we're certain all the PTE updates
@@ -2449,7 +2444,7 @@ static void gen6_ggtt_insert_entries(struct i915_address_space *vm,
 
 	for_each_sg_page(st->sgl, &sg_iter, st->nents, 0) {
 		addr = sg_page_iter_dma_address(&sg_iter);
-		iowrite32(vm->pte_encode(addr, level, true, flags), &gtt_entries[i]);
+		iowrite32(vm->pte_encode(addr, level, flags), &gtt_entries[i]);
 		i++;
 	}
 
@@ -2461,7 +2456,7 @@ static void gen6_ggtt_insert_entries(struct i915_address_space *vm,
 	 */
 	if (i != 0) {
 		unsigned long gtt = readl(&gtt_entries[i-1]);
-		WARN_ON(gtt != vm->pte_encode(addr, level, true, flags));
+		WARN_ON(gtt != vm->pte_encode(addr, level, flags));
 	}
 
 	/* This next bit makes the above posting read even more important. We
@@ -2475,9 +2470,7 @@ static void gen6_ggtt_insert_entries(struct i915_address_space *vm,
 }
 
 static void gen8_ggtt_clear_range(struct i915_address_space *vm,
-				  uint64_t start,
-				  uint64_t length,
-				  bool use_scratch)
+				  uint64_t start, uint64_t length)
 {
 	struct drm_i915_private *dev_priv = to_i915(vm->dev);
 	struct i915_ggtt *ggtt = &dev_priv->ggtt;
@@ -2497,8 +2490,7 @@ static void gen8_ggtt_clear_range(struct i915_address_space *vm,
 		num_entries = max_entries;
 
 	scratch_pte = gen8_pte_encode(px_dma(vm->scratch_page),
-				      I915_CACHE_LLC,
-				      use_scratch);
+				      I915_CACHE_LLC);
 	for (i = 0; i < num_entries; i++)
 		gen8_set_pte(&gtt_base[i], scratch_pte);
 	readl(gtt_base);
@@ -2508,8 +2500,7 @@ static void gen8_ggtt_clear_range(struct i915_address_space *vm,
 
 static void gen6_ggtt_clear_range(struct i915_address_space *vm,
 				  uint64_t start,
-				  uint64_t length,
-				  bool use_scratch)
+				  uint64_t length)
 {
 	struct drm_i915_private *dev_priv = to_i915(vm->dev);
 	struct i915_ggtt *ggtt = &dev_priv->ggtt;
@@ -2529,7 +2520,7 @@ static void gen6_ggtt_clear_range(struct i915_address_space *vm,
 		num_entries = max_entries;
 
 	scratch_pte = vm->pte_encode(px_dma(vm->scratch_page),
-				     I915_CACHE_LLC, use_scratch, 0);
+				     I915_CACHE_LLC, 0);
 
 	for (i = 0; i < num_entries; i++)
 		iowrite32(scratch_pte, &gtt_base[i]);
@@ -2558,8 +2549,7 @@ static void i915_ggtt_insert_entries(struct i915_address_space *vm,
 
 static void i915_ggtt_clear_range(struct i915_address_space *vm,
 				  uint64_t start,
-				  uint64_t length,
-				  bool unused)
+				  uint64_t length)
 {
 	struct drm_i915_private *dev_priv = vm->dev->dev_private;
 	unsigned first_entry = start >> PAGE_SHIFT;
@@ -2651,8 +2641,7 @@ static void ggtt_unbind_vma(struct i915_vma *vma)
 	if (vma->bound & GLOBAL_BIND) {
 		vma->vm->clear_range(vma->vm,
 				     vma->node.start,
-				     size,
-				     true);
+				     size);
 	}
 
 	if (dev_priv->mm.aliasing_ppgtt && vma->bound & LOCAL_BIND) {
@@ -2660,8 +2649,7 @@ static void ggtt_unbind_vma(struct i915_vma *vma)
 
 		appgtt->base.clear_range(&appgtt->base,
 					 vma->node.start,
-					 size,
-					 true);
+					 size);
 	}
 }
 
@@ -2759,11 +2747,11 @@ static int i915_gem_setup_global_gtt(struct drm_device *dev,
 		DRM_DEBUG_KMS("clearing unused GTT space: [%lx, %lx]\n",
 			      hole_start, hole_end);
 		ggtt->base.clear_range(&ggtt->base, hole_start,
-				     hole_end - hole_start, true);
+				       hole_end - hole_start);
 	}
 
 	/* And finally clear the reserved guard page */
-	ggtt->base.clear_range(&ggtt->base, end - PAGE_SIZE, PAGE_SIZE, true);
+	ggtt->base.clear_range(&ggtt->base, end - PAGE_SIZE, PAGE_SIZE);
 
 	if (USES_PPGTT(dev) && !USES_FULL_PPGTT(dev)) {
 		struct i915_hw_ppgtt *ppgtt;
@@ -2790,8 +2778,7 @@ static int i915_gem_setup_global_gtt(struct drm_device *dev,
 
 		ppgtt->base.clear_range(&ppgtt->base,
 					ppgtt->base.start,
-					ppgtt->base.total,
-					true);
+					ppgtt->base.total);
 
 		dev_priv->mm.aliasing_ppgtt = ppgtt;
 		WARN_ON(ggtt->base.bind_vma != ggtt_bind_vma);
@@ -3255,8 +3242,7 @@ void i915_gem_restore_gtt_mappings(struct drm_device *dev)
 	i915_check_and_clear_faults(dev);
 
 	/* First fill our portion of the GTT with scratch pages */
-	ggtt->base.clear_range(&ggtt->base, ggtt->base.start, ggtt->base.total,
-			       true);
+	ggtt->base.clear_range(&ggtt->base, ggtt->base.start, ggtt->base.total);
 
 	/* Cache flush objects bound into GGTT and rebind them. */
 	list_for_each_entry(obj, &dev_priv->mm.bound_list, global_list) {
diff --git a/drivers/gpu/drm/i915/i915_gem_gtt.h b/drivers/gpu/drm/i915/i915_gem_gtt.h
index 0008543..0bb4a8e 100644
--- a/drivers/gpu/drm/i915/i915_gem_gtt.h
+++ b/drivers/gpu/drm/i915/i915_gem_gtt.h
@@ -306,7 +306,7 @@ struct i915_address_space {
 	/* FIXME: Need a more generic return type */
 	gen6_pte_t (*pte_encode)(dma_addr_t addr,
 				 enum i915_cache_level level,
-				 bool valid, u32 flags); /* Create a valid PTE */
+				 u32 flags); /* Create a valid PTE */
 	/* flags for pte_encode */
 #define PTE_READ_ONLY	(1<<0)
 	int (*allocate_va_range)(struct i915_address_space *vm,
@@ -314,8 +314,7 @@ struct i915_address_space {
 				 uint64_t length);
 	void (*clear_range)(struct i915_address_space *vm,
 			    uint64_t start,
-			    uint64_t length,
-			    bool use_scratch);
+			    uint64_t length);
 	void (*insert_entries)(struct i915_address_space *vm,
 			       struct sg_table *st,
 			       uint64_t start,
-- 
1.7.1


From 5bd601d78b50ff383ec0bbccdc1f85568bddc747 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Micha=C5=82=20Winiarski?= <michal.winiarski@intel.com>
Date: Thu, 13 Oct 2016 14:02:41 +0200
Subject: [PATCH 10/17] drm/i915/gtt: Split gen8_ppgtt_clear_pte_range
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Let's use more top-down approach, where each gen8_ppgtt_clear_* function
is responsible for clearing the struct passed as an argument and calling
relevant clear_range functions on lower-level tables.
Doing this rather than operating on PTE ranges makes the implementation
of shrinking page tables quite simple.

v2: Drop min when calculating num_entries, no negation in 48b ppgtt
check, no newlines in vars block (Joonas)

Cc: Chris Wilson <chris@chris-wilson.co.uk>
Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
Cc: Michel Thierry <michel.thierry@intel.com>
Cc: Mika Kuoppala <mika.kuoppala@intel.com>
Signed-off-by: Michał Winiarski <michal.winiarski@intel.com>
Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
Link: http://patchwork.freedesktop.org/patch/msgid/1476360162-24062-2-git-send-email-michal.winiarski@intel.com

Conflicts:
	drivers/gpu/drm/i915/i915_gem_gtt.c
---
 drivers/gpu/drm/i915/i915_gem_gtt.c |  107 +++++++++++++++++++----------------
 1 files changed, 58 insertions(+), 49 deletions(-)

diff --git a/drivers/gpu/drm/i915/i915_gem_gtt.c b/drivers/gpu/drm/i915/i915_gem_gtt.c
index 3a121bc..8cfb3dd 100644
--- a/drivers/gpu/drm/i915/i915_gem_gtt.c
+++ b/drivers/gpu/drm/i915/i915_gem_gtt.c
@@ -698,59 +698,78 @@ static int gen8_48b_mm_switch(struct i915_hw_ppgtt *ppgtt,
 	return gen8_write_pdp(req, 0, px_dma(&ppgtt->pml4));
 }
 
-static void gen8_ppgtt_clear_pte_range(struct i915_address_space *vm,
-				       struct i915_page_directory_pointer *pdp,
-				       uint64_t start,
-				       uint64_t length,
-				       gen8_pte_t scratch_pte)
+static void gen8_ppgtt_clear_pt(struct i915_address_space *vm,
+				struct i915_page_table *pt,
+				uint64_t start,
+				uint64_t length)
 {
 	struct i915_hw_ppgtt *ppgtt = i915_vm_to_ppgtt(vm);
+
+	unsigned int pte_start = gen8_pte_index(start);
+	unsigned int num_entries = gen8_pte_count(start, length);
+	uint64_t pte;
 	gen8_pte_t *pt_vaddr;
-	unsigned pdpe = gen8_pdpe_index(start);
-	unsigned pde = gen8_pde_index(start);
-	unsigned pte = gen8_pte_index(start);
-	unsigned num_entries = length >> PAGE_SHIFT;
-	unsigned last_pte, i;
+	gen8_pte_t scratch_pte = gen8_pte_encode(px_dma(vm->scratch_page),
+						 I915_CACHE_LLC);
 
-	if (WARN_ON(!pdp))
+	if (WARN_ON(!px_page(pt)))
 		return;
 
-	while (num_entries) {
-		struct i915_page_directory *pd;
-		struct i915_page_table *pt;
+	bitmap_clear(pt->used_ptes, pte_start, num_entries);
 
-		if (WARN_ON(!pdp->page_directory[pdpe]))
-			break;
+	pt_vaddr = kmap_px(pt);
+
+	for (pte = pte_start; pte < num_entries; pte++)
+		pt_vaddr[pte] = scratch_pte;
 
-		pd = pdp->page_directory[pdpe];
+	kunmap_px(ppgtt, pt_vaddr);
+}
+
+static void gen8_ppgtt_clear_pd(struct i915_address_space *vm,
+				struct i915_page_directory *pd,
+				uint64_t start,
+				uint64_t length)
+{
+	struct i915_page_table *pt;
+	uint64_t pde;
 
+	gen8_for_each_pde(pt, pd, start, length, pde) {
 		if (WARN_ON(!pd->page_table[pde]))
 			break;
 
-		pt = pd->page_table[pde];
+		gen8_ppgtt_clear_pt(vm, pt, start, length);
+	}
+}
 
-		if (WARN_ON(!px_page(pt)))
-			break;
+static void gen8_ppgtt_clear_pdp(struct i915_address_space *vm,
+				 struct i915_page_directory_pointer *pdp,
+				 uint64_t start,
+				 uint64_t length)
+{
+	struct i915_page_directory *pd;
+	uint64_t pdpe;
 
-		last_pte = pte + num_entries;
-		if (last_pte > GEN8_PTES)
-			last_pte = GEN8_PTES;
+	gen8_for_each_pdpe(pd, pdp, start, length, pdpe) {
+		if (WARN_ON(!pdp->page_directory[pdpe]))
+			break;
 
-		pt_vaddr = kmap_px(pt);
+		gen8_ppgtt_clear_pd(vm, pd, start, length);
+	}
+}
 
-		for (i = pte; i < last_pte; i++) {
-			pt_vaddr[i] = scratch_pte;
-			num_entries--;
-		}
+static void gen8_ppgtt_clear_pml4(struct i915_address_space *vm,
+				  struct i915_pml4 *pml4,
+				  uint64_t start,
+				  uint64_t length)
+{
+	struct i915_page_directory_pointer *pdp;
+	uint64_t pml4e;
 
-		kunmap_px(ppgtt, pt_vaddr);
+	gen8_for_each_pml4e(pdp, pml4, start, length, pml4e) {
+		if (WARN_ON(!pml4->pdps[pml4e]))
+			break;
 
-		pte = 0;
-		if (++pde == I915_PDES) {
-			if (++pdpe == I915_PDPES_PER_PDP(vm->dev))
-				break;
-			pde = 0;
-		}
+		gen8_ppgtt_clear_pdp(vm, pdp, start, length);
 	}
 }
 
@@ -758,21 +777,11 @@ static void gen8_ppgtt_clear_range(struct i915_address_space *vm,
 				   uint64_t start, uint64_t length)
 {
 	struct i915_hw_ppgtt *ppgtt = i915_vm_to_ppgtt(vm);
-	gen8_pte_t scratch_pte = gen8_pte_encode(px_dma(vm->scratch_page),
-						 I915_CACHE_LLC);
 
-	if (!USES_FULL_48BIT_PPGTT(vm->dev)) {
-		gen8_ppgtt_clear_pte_range(vm, &ppgtt->pdp, start, length,
-					   scratch_pte);
-	} else {
-		uint64_t pml4e;
-		struct i915_page_directory_pointer *pdp;
-
-		gen8_for_each_pml4e(pdp, &ppgtt->pml4, start, length, pml4e) {
-			gen8_ppgtt_clear_pte_range(vm, pdp, start, length,
-						   scratch_pte);
-		}
-	}
+	if (USES_FULL_48BIT_PPGTT(vm->dev))
+		gen8_ppgtt_clear_pml4(vm, &ppgtt->pml4, start, length);
+	else
+		gen8_ppgtt_clear_pdp(vm, &ppgtt->pdp, start, length);
 }
 
 static void
-- 
1.7.1


From 10bf9f49749d7ba2783eb377500c7a81047d1443 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Micha=C5=82=20Winiarski?= <michal.winiarski@intel.com>
Date: Thu, 13 Oct 2016 14:02:42 +0200
Subject: [PATCH 11/17] drm/i915/gtt: Free unused lower-level page tables
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Since "Dynamic page table allocations" were introduced, our page tables
can grow (being dynamically allocated) with address space range usage.
Unfortunately, their lifetime is bound to vm. This is not a huge problem
when we're not using softpin - drm_mm is creating an upper bound on used
range by causing addresses for our VMAs to eventually be reused.

With softpin, long lived contexts can drain the system out of memory
even with a single "small" object. For example:

bo = bo_alloc(size);
while(true)
    offset += size;
    exec(bo, offset);

Will cause us to create new allocations until all memory in the system
is used for tracking GPU pages (even though almost all PTEs in this vm
are pointing to scratch).

Let's free unused page tables in clear_range to prevent this - if no
entries are used, we can safely free it and return this information to
the caller (so that higher-level entry is pointing to scratch).

v2: Document return value and free semantics (Joonas)
v3: No newlines in vars block (Joonas)
v4: Drop redundant local 'reduce' variable
v5: Handle CI fail with enable_ppgtt=2

Cc: Michel Thierry <michel.thierry@intel.com>
Cc: Mika Kuoppala <mika.kuoppala@intel.com>
Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
Signed-off-by: Michał Winiarski <michal.winiarski@intel.com>
Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
Link: http://patchwork.freedesktop.org/patch/msgid/1476360162-24062-3-git-send-email-michal.winiarski@intel.com
---
 drivers/gpu/drm/i915/i915_gem_gtt.c |   81 +++++++++++++++++++++++++++++++---
 1 files changed, 73 insertions(+), 8 deletions(-)

diff --git a/drivers/gpu/drm/i915/i915_gem_gtt.c b/drivers/gpu/drm/i915/i915_gem_gtt.c
index 8cfb3dd..3179a36 100644
--- a/drivers/gpu/drm/i915/i915_gem_gtt.c
+++ b/drivers/gpu/drm/i915/i915_gem_gtt.c
@@ -698,13 +698,15 @@ static int gen8_48b_mm_switch(struct i915_hw_ppgtt *ppgtt,
 	return gen8_write_pdp(req, 0, px_dma(&ppgtt->pml4));
 }
 
-static void gen8_ppgtt_clear_pt(struct i915_address_space *vm,
+/* Removes entries from a single page table, releasing it if it's empty.
+ * Caller can use the return value to update higher-level entries.
+ */
+static bool gen8_ppgtt_clear_pt(struct i915_address_space *vm,
 				struct i915_page_table *pt,
 				uint64_t start,
 				uint64_t length)
 {
 	struct i915_hw_ppgtt *ppgtt = i915_vm_to_ppgtt(vm);
-
 	unsigned int pte_start = gen8_pte_index(start);
 	unsigned int num_entries = gen8_pte_count(start, length);
 	uint64_t pte;
@@ -713,63 +715,126 @@ static void gen8_ppgtt_clear_pt(struct i915_address_space *vm,
 						 I915_CACHE_LLC);
 
 	if (WARN_ON(!px_page(pt)))
-		return;
+		return false;
 
 	bitmap_clear(pt->used_ptes, pte_start, num_entries);
 
+	if (bitmap_empty(pt->used_ptes, GEN8_PTES)) {
+		free_pt(vm->dev, pt);
+		return true;
+	}
+
 	pt_vaddr = kmap_px(pt);
 
 	for (pte = pte_start; pte < num_entries; pte++)
 		pt_vaddr[pte] = scratch_pte;
 
 	kunmap_px(ppgtt, pt_vaddr);
+
+	return false;
 }
 
-static void gen8_ppgtt_clear_pd(struct i915_address_space *vm,
+/* Removes entries from a single page dir, releasing it if it's empty.
+ * Caller can use the return value to update higher-level entries
+ */
+static bool gen8_ppgtt_clear_pd(struct i915_address_space *vm,
 				struct i915_page_directory *pd,
 				uint64_t start,
 				uint64_t length)
 {
+	struct i915_hw_ppgtt *ppgtt = i915_vm_to_ppgtt(vm);
 	struct i915_page_table *pt;
 	uint64_t pde;
+	gen8_pde_t *pde_vaddr;
+	gen8_pde_t scratch_pde = gen8_pde_encode(px_dma(vm->scratch_pt),
+						 I915_CACHE_LLC);
 
 	gen8_for_each_pde(pt, pd, start, length, pde) {
 		if (WARN_ON(!pd->page_table[pde]))
 			break;
 
-		gen8_ppgtt_clear_pt(vm, pt, start, length);
+		if (gen8_ppgtt_clear_pt(vm, pt, start, length)) {
+			__clear_bit(pde, pd->used_pdes);
+			pde_vaddr = kmap_px(pd);
+			pde_vaddr[pde] = scratch_pde;
+			kunmap_px(ppgtt, pde_vaddr);
+		}
+	}
+
+	if (bitmap_empty(pd->used_pdes, I915_PDES)) {
+		free_pd(vm->dev, pd);
+		return true;
 	}
+
+	return false;
 }
 
-static void gen8_ppgtt_clear_pdp(struct i915_address_space *vm,
+/* Removes entries from a single page dir pointer, releasing it if it's empty.
+ * Caller can use the return value to update higher-level entries
+ */
+static bool gen8_ppgtt_clear_pdp(struct i915_address_space *vm,
 				 struct i915_page_directory_pointer *pdp,
 				 uint64_t start,
 				 uint64_t length)
 {
+	struct i915_hw_ppgtt *ppgtt = i915_vm_to_ppgtt(vm);
 	struct i915_page_directory *pd;
 	uint64_t pdpe;
+	gen8_ppgtt_pdpe_t *pdpe_vaddr;
+	gen8_ppgtt_pdpe_t scratch_pdpe =
+		gen8_pdpe_encode(px_dma(vm->scratch_pd), I915_CACHE_LLC);
 
 	gen8_for_each_pdpe(pd, pdp, start, length, pdpe) {
 		if (WARN_ON(!pdp->page_directory[pdpe]))
 			break;
 
-		gen8_ppgtt_clear_pd(vm, pd, start, length);
+		if (gen8_ppgtt_clear_pd(vm, pd, start, length)) {
+			__clear_bit(pdpe, pdp->used_pdpes);
+			if (USES_FULL_48BIT_PPGTT(vm->dev)) {
+				pdpe_vaddr = kmap_px(pdp);
+				pdpe_vaddr[pdpe] = scratch_pdpe;
+				kunmap_px(ppgtt, pdpe_vaddr);
+			}
+		}
 	}
+
+	if (USES_FULL_48BIT_PPGTT(vm->dev) &&
+	    bitmap_empty(pdp->used_pdpes, I915_PDPES_PER_PDP(vm->dev))) {
+		free_pdp(vm->dev, pdp);
+		return true;
+	}
+
+	return false;
 }
 
+/* Removes entries from a single pml4.
+ * This is the top-level structure in 4-level page tables used on gen8+.
+ * Empty entries are always scratch pml4e.
+ */
 static void gen8_ppgtt_clear_pml4(struct i915_address_space *vm,
 				  struct i915_pml4 *pml4,
 				  uint64_t start,
 				  uint64_t length)
 {
+	struct i915_hw_ppgtt *ppgtt = i915_vm_to_ppgtt(vm);
 	struct i915_page_directory_pointer *pdp;
 	uint64_t pml4e;
+	gen8_ppgtt_pml4e_t *pml4e_vaddr;
+	gen8_ppgtt_pml4e_t scratch_pml4e =
+		gen8_pml4e_encode(px_dma(vm->scratch_pdp), I915_CACHE_LLC);
+
+	GEM_BUG_ON(!USES_FULL_48BIT_PPGTT(vm->dev));
 
 	gen8_for_each_pml4e(pdp, pml4, start, length, pml4e) {
 		if (WARN_ON(!pml4->pdps[pml4e]))
 			break;
 
-		gen8_ppgtt_clear_pdp(vm, pdp, start, length);
+		if (gen8_ppgtt_clear_pdp(vm, pdp, start, length)) {
+			__clear_bit(pml4e, pml4->used_pml4es);
+			pml4e_vaddr = kmap_px(pml4);
+			pml4e_vaddr[pml4e] = scratch_pml4e;
+			kunmap_px(ppgtt, pml4e_vaddr);
+		}
 	}
 }
 
-- 
1.7.1


From 2f0a31ac24d6e8437303e67d698c255534a03f41 Mon Sep 17 00:00:00 2001
From: Mika Kuoppala <mika.kuoppala@linux.intel.com>
Date: Mon, 31 Oct 2016 17:24:46 +0200
Subject: [PATCH 12/17] drm/i915/gtt: Mark tlbs dirty on clear

Now when clearing ptes can modify upper level pdp's,
we need to mark them dirty so that they will be flushed
correctly.

Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
Signed-off-by: Mika Kuoppala <mika.kuoppala@intel.com>
Link: http://patchwork.freedesktop.org/patch/msgid/1478006856-8313-1-git-send-email-mika.kuoppala@intel.com
---
 drivers/gpu/drm/i915/i915_gem_gtt.c |   22 ++++++++++++----------
 1 files changed, 12 insertions(+), 10 deletions(-)

diff --git a/drivers/gpu/drm/i915/i915_gem_gtt.c b/drivers/gpu/drm/i915/i915_gem_gtt.c
index 3179a36..e42098c 100644
--- a/drivers/gpu/drm/i915/i915_gem_gtt.c
+++ b/drivers/gpu/drm/i915/i915_gem_gtt.c
@@ -698,6 +698,16 @@ static int gen8_48b_mm_switch(struct i915_hw_ppgtt *ppgtt,
 	return gen8_write_pdp(req, 0, px_dma(&ppgtt->pml4));
 }
 
+/* PDE TLBs are a pain to invalidate on GEN8+. When we modify
+ * the page table structures, we mark them dirty so that
+ * context switching/execlist queuing code takes extra steps
+ * to ensure that tlbs are flushed.
+ */
+static void mark_tlbs_dirty(struct i915_hw_ppgtt *ppgtt)
+{
+	ppgtt->pd_dirty_rings = INTEL_INFO(ppgtt->base.dev)->ring_mask;
+}
+
 /* Removes entries from a single page table, releasing it if it's empty.
  * Caller can use the return value to update higher-level entries.
  */
@@ -798,6 +808,8 @@ static bool gen8_ppgtt_clear_pdp(struct i915_address_space *vm,
 		}
 	}
 
+	mark_tlbs_dirty(ppgtt);
+
 	if (USES_FULL_48BIT_PPGTT(vm->dev) &&
 	    bitmap_empty(pdp->used_pdpes, I915_PDPES_PER_PDP(vm->dev))) {
 		free_pdp(vm->dev, pdp);
@@ -1265,16 +1277,6 @@ err_out:
 	return -ENOMEM;
 }
 
-/* PDE TLBs are a pain to invalidate on GEN8+. When we modify
- * the page table structures, we mark them dirty so that
- * context switching/execlist queuing code takes extra steps
- * to ensure that tlbs are flushed.
- */
-static void mark_tlbs_dirty(struct i915_hw_ppgtt *ppgtt)
-{
-	ppgtt->pd_dirty_rings = INTEL_INFO(ppgtt->base.dev)->ring_mask;
-}
-
 static int gen8_alloc_va_range_3lvl(struct i915_address_space *vm,
 				    struct i915_page_directory_pointer *pdp,
 				    uint64_t start,
-- 
1.7.1


From be863ebf4531ba2edd89e04ac83b5accb830ffb3 Mon Sep 17 00:00:00 2001
From: Mika Kuoppala <mika.kuoppala@linux.intel.com>
Date: Tue, 1 Nov 2016 15:27:36 +0200
Subject: [PATCH 13/17] drm/i915/gtt: Fix pte clear range
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Comparing pte index to a number of entries is wrong
when clearing a range of pte entries. Use end marker
of 'one past' to correctly point adequate number of
ptes to the scratch page.

v2: assert early instead of warning late (Chris)
v3: removed consts (Joonas)

Fixes: d209b9c3cd28 ("drm/i915/gtt: Split gen8_ppgtt_clear_pte_range")
Bugzilla: https://bugs.freedesktop.org/show_bug.cgi?id=98282
Cc: Chris Wilson <chris@chris-wilson.co.uk>
Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
Cc: Michel Thierry <michel.thierry@intel.com>
Cc: Michał Winiarski <michal.winiarski@intel.com>
Reported-by: Mike Lothian <mike@fireburn.co.uk>
Signed-off-by: Mika Kuoppala <mika.kuoppala@intel.com>
Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
Tested-by: Mike Lothian <mike@fireburn.co.uk>
Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
Signed-off-by: Mika Kuoppala <mika.kuoppala@intel.com>
---
 drivers/gpu/drm/i915/i915_gem_gtt.c |   12 +++++++-----
 1 files changed, 7 insertions(+), 5 deletions(-)

diff --git a/drivers/gpu/drm/i915/i915_gem_gtt.c b/drivers/gpu/drm/i915/i915_gem_gtt.c
index e42098c..b279c0b 100644
--- a/drivers/gpu/drm/i915/i915_gem_gtt.c
+++ b/drivers/gpu/drm/i915/i915_gem_gtt.c
@@ -717,9 +717,9 @@ static bool gen8_ppgtt_clear_pt(struct i915_address_space *vm,
 				uint64_t length)
 {
 	struct i915_hw_ppgtt *ppgtt = i915_vm_to_ppgtt(vm);
-	unsigned int pte_start = gen8_pte_index(start);
 	unsigned int num_entries = gen8_pte_count(start, length);
-	uint64_t pte;
+	unsigned int pte = gen8_pte_index(start);
+	unsigned int pte_end = pte + num_entries;
 	gen8_pte_t *pt_vaddr;
 	gen8_pte_t scratch_pte = gen8_pte_encode(px_dma(vm->scratch_page),
 						 I915_CACHE_LLC);
@@ -727,7 +727,9 @@ static bool gen8_ppgtt_clear_pt(struct i915_address_space *vm,
 	if (WARN_ON(!px_page(pt)))
 		return false;
 
-	bitmap_clear(pt->used_ptes, pte_start, num_entries);
+	GEM_BUG_ON(pte_end > GEN8_PTES);
+
+	bitmap_clear(pt->used_ptes, pte, num_entries);
 
 	if (bitmap_empty(pt->used_ptes, GEN8_PTES)) {
 		free_pt(vm->dev, pt);
@@ -736,8 +738,8 @@ static bool gen8_ppgtt_clear_pt(struct i915_address_space *vm,
 
 	pt_vaddr = kmap_px(pt);
 
-	for (pte = pte_start; pte < num_entries; pte++)
-		pt_vaddr[pte] = scratch_pte;
+	while (pte < pte_end)
+		pt_vaddr[pte++] = scratch_pte;
 
 	kunmap_px(ppgtt, pt_vaddr);
 
-- 
1.7.1


From a7eabd2219e65cd83404c923345c7fce81b3f2a1 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Micha=C5=82=20Winiarski?= <michal.winiarski@intel.com>
Date: Fri, 15 Apr 2016 14:14:02 +0200
Subject: [PATCH 14/17] [VPG]: drm/i915: Add support for OCL Turbo Boost

For some OpenCL workloads, the PM interrupts that drive RPS to increase
GPU frequency are not generated, resulting in workload executing at
low frequency.

Let's implement a new private context param, controling whether the GPU
frequency should be increased to max softlimit on request submission.
After the initial GPU frequency increase, we're making sure that
subsequent PM interrupts driving the frequency down are ignored by the
RPS for the period of 2000ms after request submission.

Cc: Zhipeng Gong <zhipeng.gong@intel.com>
Cc: John Harrison <John.C.Harrison@Intel.com>
---
 drivers/gpu/drm/i915/i915_drv.h            |    8 +++++
 drivers/gpu/drm/i915/i915_gem_context.c    |   41 ++++++++++++++++++++++++++++
 drivers/gpu/drm/i915/i915_gem_execbuffer.c |    3 ++
 drivers/gpu/drm/i915/i915_irq.c            |    2 +-
 drivers/gpu/drm/i915/intel_pm.c            |   16 +++++++++++
 include/uapi/drm/i915_drm.h                |    1 +
 6 files changed, 70 insertions(+), 1 deletions(-)

diff --git a/drivers/gpu/drm/i915/i915_drv.h b/drivers/gpu/drm/i915/i915_drv.h
index d425a70..eabedbb 100644
--- a/drivers/gpu/drm/i915/i915_drv.h
+++ b/drivers/gpu/drm/i915/i915_drv.h
@@ -825,6 +825,7 @@ struct i915_ctx_hang_stats {
 #define DEFAULT_CONTEXT_HANDLE 0
 
 #define CONTEXT_NO_ZEROMAP (1<<0)
+#define CONTEXT_BOOST_FREQ (1<<31)
 /**
  * struct intel_context - as the name implies, represents a context.
  * @ref: reference count.
@@ -1170,6 +1171,13 @@ struct intel_gen6_power_mgmt {
 	 * talking to hw - so only take it when talking to hw!
 	 */
 	struct mutex hw_lock;
+
+#define DRM_I915_BOOST_TIMEOUT 2000
+#define DRM_I915_BOOST_TIMEOUT_JIFFIES msecs_to_jiffies(DRM_I915_BOOST_TIMEOUT)
+	struct timer_list boost_timeout;
+
+	atomic_t use_boost_freq;
+	atomic_t boost_ctx_count;
 };
 
 /* defined intel_pm.c */
diff --git a/drivers/gpu/drm/i915/i915_gem_context.c b/drivers/gpu/drm/i915/i915_gem_context.c
index a8aeec9..ab0dbbd 100644
--- a/drivers/gpu/drm/i915/i915_gem_context.c
+++ b/drivers/gpu/drm/i915/i915_gem_context.c
@@ -148,6 +148,22 @@ static void i915_gem_context_clean(struct intel_context *ctx)
 	}
 }
 
+static inline void i915_gem_context_boost_inc(struct intel_context *ctx)
+{
+	struct intel_gen6_power_mgmt *rps = &ctx->i915->rps;
+	atomic_inc(&rps->boost_ctx_count);
+}
+
+static inline void i915_gem_context_boost_dec(struct intel_context *ctx)
+{
+	struct intel_gen6_power_mgmt *rps = &ctx->i915->rps;
+	WARN_ON(!atomic_read(&rps->boost_ctx_count));
+	if (atomic_dec_and_test(&rps->boost_ctx_count))
+		if (del_timer(&rps->boost_timeout))
+			atomic_set(&rps->use_boost_freq, 0);
+
+}
+
 void i915_gem_context_free(struct kref *ctx_ref)
 {
 	struct intel_context *ctx = container_of(ctx_ref, typeof(*ctx), ref);
@@ -171,6 +187,8 @@ void i915_gem_context_free(struct kref *ctx_ref)
 
 	put_pid(ctx->pid);
 	i915_perfmon_ctx_cleanup(ctx);
+	if (ctx->flags & CONTEXT_BOOST_FREQ)
+		i915_gem_context_boost_dec(ctx);
 
 	list_del(&ctx->link);
 	kfree(ctx);
@@ -957,6 +975,8 @@ int i915_gem_context_getparam_ioctl(struct drm_device *dev, void *data,
 			args->value = to_i915(dev)->mm.aliasing_ppgtt->base.total;
 		else
 			args->value = to_i915(dev)->ggtt.base.total;
+	case I915_CONTEXT_PRIVATE_PARAM_BOOST:
+		args->value = ctx->flags & CONTEXT_BOOST_FREQ;
 		break;
 	default:
 		ret = -EINVAL;
@@ -1003,6 +1023,27 @@ int i915_gem_context_setparam_ioctl(struct drm_device *dev, void *data,
 			ctx->flags |= args->value ? CONTEXT_NO_ZEROMAP : 0;
 		}
 		break;
+
+	case I915_CONTEXT_PRIVATE_PARAM_BOOST:
+	{
+		int val = !!args->value;
+		if (args->size)
+			ret = -EINVAL;
+		else {
+			if (val != (ctx->flags & CONTEXT_BOOST_FREQ)) {
+				if (val) {
+					ctx->flags |= CONTEXT_BOOST_FREQ;
+					i915_gem_context_boost_inc(ctx);
+				}
+				else {
+					ctx->flags &= ~CONTEXT_NO_ZEROMAP;
+					i915_gem_context_boost_dec(ctx);
+				}
+			}
+		}
+		break;
+	}
+
 	default:
 		ret = -EINVAL;
 		break;
diff --git a/drivers/gpu/drm/i915/i915_gem_execbuffer.c b/drivers/gpu/drm/i915/i915_gem_execbuffer.c
index 4629eda..e07edbf 100644
--- a/drivers/gpu/drm/i915/i915_gem_execbuffer.c
+++ b/drivers/gpu/drm/i915/i915_gem_execbuffer.c
@@ -1645,6 +1645,9 @@ i915_gem_do_execbuffer(struct drm_device *dev, void *data,
 err_request:
 	i915_gem_execbuffer_retire_commands(params);
 
+	if (ctx->flags & CONTEXT_BOOST_FREQ)
+		intel_queue_rps_boost_for_request(dev, params->request);
+
 err_batch_unpin:
 	/*
 	 * FIXME: We crucially rely upon the active tracking for the (ppgtt)
diff --git a/drivers/gpu/drm/i915/i915_irq.c b/drivers/gpu/drm/i915/i915_irq.c
index aab47f7..81155e9 100644
--- a/drivers/gpu/drm/i915/i915_irq.c
+++ b/drivers/gpu/drm/i915/i915_irq.c
@@ -1134,7 +1134,7 @@ static void gen6_pm_rps_work(struct work_struct *work)
 	min = dev_priv->rps.min_freq_softlimit;
 	max = dev_priv->rps.max_freq_softlimit;
 
-	if (client_boost) {
+	if (client_boost || atomic_read(&dev_priv->rps.use_boost_freq)) {
 		new_delay = dev_priv->rps.max_freq_softlimit;
 		adj = 0;
 	} else if (pm_iir & GEN6_PM_RP_UP_THRESHOLD) {
diff --git a/drivers/gpu/drm/i915/intel_pm.c b/drivers/gpu/drm/i915/intel_pm.c
index 2863b92..7fd8ed3 100644
--- a/drivers/gpu/drm/i915/intel_pm.c
+++ b/drivers/gpu/drm/i915/intel_pm.c
@@ -7477,6 +7477,18 @@ void intel_queue_rps_boost_for_request(struct drm_device *dev,
 
 	INIT_WORK(&boost->work, __intel_rps_boost_work);
 	queue_work(to_i915(dev)->wq, &boost->work);
+
+	if (req->ctx->flags & CONTEXT_BOOST_FREQ) {
+		atomic_set(&to_i915(dev)->rps.use_boost_freq, 1);
+		mod_timer(&to_i915(dev)->rps.boost_timeout,
+			  req->emitted_jiffies + DRM_I915_BOOST_TIMEOUT_JIFFIES);
+	}
+}
+
+static void intel_boost_timeout_handler(unsigned long data)
+{
+	atomic_t *use_boost_freq = (atomic_t *)data;
+	atomic_set(use_boost_freq, 0);
 }
 
 void intel_pm_setup(struct drm_device *dev)
@@ -7491,6 +7503,10 @@ void intel_pm_setup(struct drm_device *dev)
 	INIT_LIST_HEAD(&dev_priv->rps.clients);
 	INIT_LIST_HEAD(&dev_priv->rps.semaphores.link);
 	INIT_LIST_HEAD(&dev_priv->rps.mmioflips.link);
+	atomic_set(&dev_priv->rps.use_boost_freq, 0);
+	atomic_set(&dev_priv->rps.boost_ctx_count, 0);
+	setup_timer(&dev_priv->rps.boost_timeout, intel_boost_timeout_handler,
+			(unsigned long)&dev_priv->rps.use_boost_freq);
 
 	dev_priv->pm.suspended = false;
 	atomic_set(&dev_priv->pm.wakeref_count, 0);
diff --git a/include/uapi/drm/i915_drm.h b/include/uapi/drm/i915_drm.h
index 20631d0..2aa86e6 100644
--- a/include/uapi/drm/i915_drm.h
+++ b/include/uapi/drm/i915_drm.h
@@ -1183,6 +1183,7 @@ struct drm_i915_gem_context_param {
 #define I915_CONTEXT_PARAM_BAN_PERIOD	0x1
 #define I915_CONTEXT_PARAM_NO_ZEROMAP	0x2
 #define I915_CONTEXT_PARAM_GTT_SIZE	0x3
+#define I915_CONTEXT_PRIVATE_PARAM_BOOST 0x80000000
 	__u64 value;
 };
 
-- 
1.7.1


From 603eeecea831d909784a1d75dc1aa0e8e8d824b4 Mon Sep 17 00:00:00 2001
From: Jacek Danecki <jacek.danecki@intel.com>
Date: Fri, 2 Dec 2016 13:58:47 +0100
Subject: [PATCH 15/17] Subject: drm/i915/bxt: Enable Pooled EU support

This mode allows to assign EUs to pools which can process work collectively.
The command to enable this mode should be issued as part of context initialization.

The pooled mode is global, once enabled it has to stay the same across all
contexts until HW reset hence this is sent in auxiliary golden context batch.
Thanks to Mika for the preliminary review and comments.

v2: explain why this is enabled in golden context, use feature flag while
enabling the support (Chris)

v3: Pooled EU support announced in userspace before enabling in kernel,
to simplify include all changes in the same patch.

User space clients need to know when the pooled EU feature is present
and enabled on the hardware so that they can adapt work submissions.
Create a new device info flag for this purpose, and create a new GETPARAM
entry to allow user space to query its setting.

Set has_pooled_eu to true in the Broxton static device info - Broxton
supports the feature in hardware and the driver will enable it by
default.

Note: This patch actually enables Pooled EU feature. The review for this
patch is completed but not merged in upstream because there is no
opensource user for this feature at this point.
http://lists.freedesktop.org/archives/intel-gfx/2015-July/072051.html

I have marked this as FROM_UPSTREAM since the review is complete and
no more changes expected for this patch.

Opensource users for this feature are mesa, libva and beignet. I have
started communication with them to see their interest, beignet team
doesn't have bxt available yet so they will be adding userspace bits
when they start working with bxt. Mesa hasn't responded yet.

Tracked-On: https://jira01.devtools.intel.com/browse/VIZ-6174
Change-Id: I7cec2b5aa03ca38c802e1a04fc2f9e16088b542c
Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
Cc: Mika Kuoppala <mika.kuoppala@intel.com>
Cc: Chris Wilson <chris@chris-wilson.co.uk>
Cc: Armin Reese <armin.c.reese@intel.com>
Signed-off-by: Jeff McGee <jeff.mcgee@intel.com>
Signed-off-by: Arun Siluvery <arun.siluvery@linux.intel.com>
Signed-off-by: kimsehun <se.hun.kim@intel.com>
[Jeff: Adjust the new GETPARAM index to start from 0x800, the
 private (not upstreamed) offset.]
Signed-off-by: Jeff McGee <jeff.mcgee@intel.com>
Signed-off-by: Matt Roper <matthew.d.roper@intel.com>
---
 drivers/gpu/drm/i915/i915_dma.c              |    3 +++
 drivers/gpu/drm/i915/i915_drv.c              |    1 +
 drivers/gpu/drm/i915/i915_drv.h              |    5 ++++-
 drivers/gpu/drm/i915/i915_gem_render_state.c |   13 +++++++++++++
 drivers/gpu/drm/i915/i915_reg.h              |    2 ++
 include/uapi/drm/i915_drm.h                  |    4 ++++
 6 files changed, 27 insertions(+), 1 deletions(-)

diff --git a/drivers/gpu/drm/i915/i915_dma.c b/drivers/gpu/drm/i915/i915_dma.c
index dd8ea26..147b200 100644
--- a/drivers/gpu/drm/i915/i915_dma.c
+++ b/drivers/gpu/drm/i915/i915_dma.c
@@ -236,6 +236,9 @@ static int i915_getparam(struct drm_device *dev, void *data,
 		value = !dev_priv->workarounds.WaForceEnableNonCoherent &&
 			INTEL_INFO(dev)->gen >= 9;
 		break;
+	case I915_PARAM_HAS_POOLED_EU:
+		value = HAS_POOLED_EU(dev);
+		break;
 	default:
 		DRM_DEBUG("Unknown parameter %d\n", param->param);
 		return -EINVAL;
diff --git a/drivers/gpu/drm/i915/i915_drv.c b/drivers/gpu/drm/i915/i915_drv.c
index 85c4deb..f6abc6c 100644
--- a/drivers/gpu/drm/i915/i915_drv.c
+++ b/drivers/gpu/drm/i915/i915_drv.c
@@ -353,6 +353,7 @@ static const struct intel_device_info intel_broxton_info = {
 	.has_ddi = 1,
 	.has_fpga_dbg = 1,
 	.has_fbc = 1,
+	.has_pooled_eu = 1,
 	GEN_DEFAULT_PIPEOFFSETS,
 	IVB_CURSOR_OFFSETS,
 	BDW_COLORS,
diff --git a/drivers/gpu/drm/i915/i915_drv.h b/drivers/gpu/drm/i915/i915_drv.h
index eabedbb..3e12630 100644
--- a/drivers/gpu/drm/i915/i915_drv.h
+++ b/drivers/gpu/drm/i915/i915_drv.h
@@ -752,7 +752,8 @@ struct intel_csr {
 	func(has_llc) sep \
 	func(has_snoop) sep \
 	func(has_ddi) sep \
-	func(has_fpga_dbg)
+	func(has_fpga_dbg) sep \
+	func(has_pooled_eu)
 
 #define DEFINE_FLAG(name) u8 name:1
 #define SEP_SEMICOLON ;
@@ -2729,6 +2730,8 @@ struct drm_i915_cmd_table {
 
 #define HAS_CSR(dev)	(IS_GEN9(dev))
 
+#define HAS_POOLED_EU(dev) (INTEL_INFO(dev)->has_pooled_eu)
+
 #define HAS_GUC_UCODE(dev)	(IS_GEN9(dev) && !IS_KABYLAKE(dev))
 #define HAS_GUC_SCHED(dev)	(IS_GEN9(dev) && !IS_KABYLAKE(dev))
 
diff --git a/drivers/gpu/drm/i915/i915_gem_render_state.c b/drivers/gpu/drm/i915/i915_gem_render_state.c
index 71611bf..da0b796 100644
--- a/drivers/gpu/drm/i915/i915_gem_render_state.c
+++ b/drivers/gpu/drm/i915/i915_gem_render_state.c
@@ -93,6 +93,7 @@ free_gem:
 
 static int render_state_setup(struct render_state *so)
 {
+	struct drm_device *dev = so->obj->base.dev;
 	const struct intel_renderstate_rodata *rodata = so->rodata;
 	unsigned int i = 0, reloc_index = 0;
 	struct page *page;
@@ -134,6 +135,18 @@ static int render_state_setup(struct render_state *so)
 
 	so->aux_batch_offset = i * sizeof(u32);
 
+	if (HAS_POOLED_EU(dev)) {
+		u32 pool_config = (INTEL_INFO(dev)->subslice_total == 3 ?
+				   0x00777000 : 0);
+
+		OUT_BATCH(d, i, GEN9_MEDIA_POOL_STATE);
+		OUT_BATCH(d, i, GEN9_MEDIA_POOL_ENABLE);
+		OUT_BATCH(d, i, pool_config);
+		OUT_BATCH(d, i, 0);
+		OUT_BATCH(d, i, 0);
+		OUT_BATCH(d, i, 0);
+	}
+
 	OUT_BATCH(d, i, MI_BATCH_BUFFER_END);
 	so->aux_batch_size = (i * sizeof(u32)) - so->aux_batch_offset;
 
diff --git a/drivers/gpu/drm/i915/i915_reg.h b/drivers/gpu/drm/i915/i915_reg.h
index 14e4b54..b899127 100644
--- a/drivers/gpu/drm/i915/i915_reg.h
+++ b/drivers/gpu/drm/i915/i915_reg.h
@@ -445,6 +445,8 @@ static inline bool i915_mmio_reg_valid(i915_reg_t reg)
  */
 #define GFX_INSTR(opcode, flags) ((0x3 << 29) | ((opcode) << 24) | (flags))
 
+#define GEN9_MEDIA_POOL_STATE     ((0x3 << 29) | (0x2 << 27) | (0x5 << 16) | 4)
+#define   GEN9_MEDIA_POOL_ENABLE  (1 << 31)
 #define GFX_OP_RASTER_RULES    ((0x3<<29)|(0x7<<24))
 #define GFX_OP_SCISSOR         ((0x3<<29)|(0x1c<<24)|(0x10<<19))
 #define   SC_UPDATE_SCISSOR       (0x1<<1)
diff --git a/include/uapi/drm/i915_drm.h b/include/uapi/drm/i915_drm.h
index 2aa86e6..41c6ed9 100644
--- a/include/uapi/drm/i915_drm.h
+++ b/include/uapi/drm/i915_drm.h
@@ -368,6 +368,10 @@ typedef struct drm_i915_irq_wait {
 
 #define I915_PRIVATE_PARAM_HAS_EXEC_FORCE_NON_COHERENT (-1)
 
+/* Private (not upstreamed) parameters start from 0x800   */
+/* This helps to avoid conflicts with new upstream values */
+#define I915_PARAM_HAS_POOLED_EU         0x800
+
 typedef struct drm_i915_getparam {
 	__s32 param;
 	/*
-- 
1.7.1


From 8b1de000ec5e37a94cf2a16d121b7570367095cf Mon Sep 17 00:00:00 2001
From: Jacek Danecki <jacek.danecki@intel.com>
Date: Fri, 2 Dec 2016 13:59:56 +0100
Subject: [PATCH 16/17] drm/i915/bxt: Add WaEnablePooledEuFor2x6

Pooled EU is enabled by default for BXT but for fused down 2x6 parts it is
advised to turn it off. But there is another HW issue in these parts (fused
down 2x6 parts) before C0 that requires Pooled EU to be enabled as a
workaround. In this case the pool configuration changes depending upon
which subslice is disabled. This doesn't affect if the device has all 3
subslices enabled.

Userspace need to know min no. of eus in a pool as it varies based on which
subslice is disabled, this is exported using a new private getparam ioctl
as there is no opensource user available for this feature yet.

Change-Id: I2f193a097a2e0354ccb29a16d57b05c6cf0d37c2
Tracked-On: https://jira01.devtools.intel.com/browse/OAM-18381
Issue: https://jira01.devtools.intel.com/browse/VIZ-7150
Signed-off-by: Arun Siluvery <arun.siluvery@linux.intel.com>
Reviewed-on: https://android.intel.com:443/459394
[Jeff: Adjust the new GETPARAM index to start from 0x800, the
 private (not upstreamed) offset.]
Signed-off-by: Jeff McGee <jeff.mcgee@intel.com>
Signed-off-by: Matt Roper <matthew.d.roper@intel.com>
---
 drivers/gpu/drm/i915/i915_debugfs.c          |    5 ++++
 drivers/gpu/drm/i915/i915_dma.c              |   33 ++++++++++++++++++++++++++
 drivers/gpu/drm/i915/i915_drv.c              |    2 +-
 drivers/gpu/drm/i915/i915_drv.h              |    1 +
 drivers/gpu/drm/i915/i915_gem_render_state.c |   17 +++++++++++--
 include/uapi/drm/i915_drm.h                  |    1 +
 6 files changed, 55 insertions(+), 4 deletions(-)

diff --git a/drivers/gpu/drm/i915/i915_debugfs.c b/drivers/gpu/drm/i915/i915_debugfs.c
index 1035468..2bc9ef3 100644
--- a/drivers/gpu/drm/i915/i915_debugfs.c
+++ b/drivers/gpu/drm/i915/i915_debugfs.c
@@ -5277,6 +5277,11 @@ static int i915_sseu_status(struct seq_file *m, void *unused)
 		   INTEL_INFO(dev)->eu_total);
 	seq_printf(m, "  Available EU Per Subslice: %u\n",
 		   INTEL_INFO(dev)->eu_per_subslice);
+	seq_printf(m, "  Has Pooled EU: %s\n",
+		   yesno(HAS_POOLED_EU(dev)));
+	if (HAS_POOLED_EU(dev))
+		seq_printf(m, "  Min EU in pool: %u\n",
+			   INTEL_INFO(dev)->min_eu_in_pool);
 	seq_printf(m, "  Has Slice Power Gating: %s\n",
 		   yesno(INTEL_INFO(dev)->has_slice_pg));
 	seq_printf(m, "  Has Subslice Power Gating: %s\n",
diff --git a/drivers/gpu/drm/i915/i915_dma.c b/drivers/gpu/drm/i915/i915_dma.c
index 147b200..5737785 100644
--- a/drivers/gpu/drm/i915/i915_dma.c
+++ b/drivers/gpu/drm/i915/i915_dma.c
@@ -239,6 +239,9 @@ static int i915_getparam(struct drm_device *dev, void *data,
 	case I915_PARAM_HAS_POOLED_EU:
 		value = HAS_POOLED_EU(dev);
 		break;
+	case I915_PARAM_MIN_EU_IN_POOL:
+		value = INTEL_INFO(dev)->min_eu_in_pool;
+		break;
 	default:
 		DRM_DEBUG("Unknown parameter %d\n", param->param);
 		return -EINVAL;
@@ -735,6 +738,32 @@ static void gen9_sseu_info_init(struct drm_device *dev)
 			       (info->slice_total > 1));
 	info->has_subslice_pg = (IS_BROXTON(dev) && (info->subslice_total > 1));
 	info->has_eu_pg = (info->eu_per_subslice > 2);
+
+	if (IS_BROXTON(dev)) {
+#define IS_SS_DISABLED(_ss_disable, ss)    (_ss_disable & (0x1 << ss))
+		/*
+		 * There is a HW issue in 2x6 fused down parts that requires
+		 * Pooled EU to be enabled as a WA. The pool configuration
+		 * changes depending upon which subslice is fused down. This
+		 * doesn't affect if the device has all 3 subslices enabled.
+		 */
+		/* WaEnablePooledEuFor2x6:bxt */
+		info->has_pooled_eu = ((info->subslice_total == 3) ||
+				       (info->subslice_total == 2 &&
+					INTEL_REVID(dev) < BXT_REVID_C0));
+
+		info->min_eu_in_pool = 0;
+		if (info->has_pooled_eu) {
+			if (IS_SS_DISABLED(ss_disable, 0) ||
+			    IS_SS_DISABLED(ss_disable, 2))
+				info->min_eu_in_pool = 3;
+			else if (IS_SS_DISABLED(ss_disable, 1))
+				info->min_eu_in_pool = 6;
+			else
+				info->min_eu_in_pool = 9;
+		}
+#undef IS_SS_DISABLED
+	}
 }
 
 static void broadwell_sseu_info_init(struct drm_device *dev)
@@ -931,6 +960,10 @@ static void intel_device_info_runtime_init(struct drm_device *dev)
 	DRM_DEBUG_DRIVER("subslice per slice: %u\n", info->subslice_per_slice);
 	DRM_DEBUG_DRIVER("EU total: %u\n", info->eu_total);
 	DRM_DEBUG_DRIVER("EU per subslice: %u\n", info->eu_per_subslice);
+	DRM_DEBUG_DRIVER("Has Pooled EU: %s\n",
+			 HAS_POOLED_EU(dev) ? "y" : "n");
+	if (HAS_POOLED_EU(dev))
+		DRM_DEBUG_DRIVER("Min EU in pool: %u\n", info->min_eu_in_pool);
 	DRM_DEBUG_DRIVER("has slice power gating: %s\n",
 			 info->has_slice_pg ? "y" : "n");
 	DRM_DEBUG_DRIVER("has subslice power gating: %s\n",
diff --git a/drivers/gpu/drm/i915/i915_drv.c b/drivers/gpu/drm/i915/i915_drv.c
index f6abc6c..729298e 100644
--- a/drivers/gpu/drm/i915/i915_drv.c
+++ b/drivers/gpu/drm/i915/i915_drv.c
@@ -353,7 +353,7 @@ static const struct intel_device_info intel_broxton_info = {
 	.has_ddi = 1,
 	.has_fpga_dbg = 1,
 	.has_fbc = 1,
-	.has_pooled_eu = 1,
+	.has_pooled_eu = 0,
 	GEN_DEFAULT_PIPEOFFSETS,
 	IVB_CURSOR_OFFSETS,
 	BDW_COLORS,
diff --git a/drivers/gpu/drm/i915/i915_drv.h b/drivers/gpu/drm/i915/i915_drv.h
index 3e12630..34eacac 100644
--- a/drivers/gpu/drm/i915/i915_drv.h
+++ b/drivers/gpu/drm/i915/i915_drv.h
@@ -778,6 +778,7 @@ struct intel_device_info {
 	u8 subslice_per_slice;
 	u8 eu_total;
 	u8 eu_per_subslice;
+	u8 min_eu_in_pool;
 	/* For each slice, which subslice(s) has(have) 7 EUs (bitfield)? */
 	u8 subslice_7eu[3];
 	u8 has_slice_pg:1;
diff --git a/drivers/gpu/drm/i915/i915_gem_render_state.c b/drivers/gpu/drm/i915/i915_gem_render_state.c
index da0b796..aa907ad 100644
--- a/drivers/gpu/drm/i915/i915_gem_render_state.c
+++ b/drivers/gpu/drm/i915/i915_gem_render_state.c
@@ -136,12 +136,23 @@ static int render_state_setup(struct render_state *so)
 	so->aux_batch_offset = i * sizeof(u32);
 
 	if (HAS_POOLED_EU(dev)) {
-		u32 pool_config = (INTEL_INFO(dev)->subslice_total == 3 ?
-				   0x00777000 : 0);
+		/*
+		 * We always program 3x6 pool config but depending upon which
+		 * subslice is disabled HW drops down to appropriate config
+		 * shown below.
+		 *
+		 * SNo  subslices config                eu pool configuration
+		 * -----------------------------------------------------------
+		 * 1    3 subslices enabled (3x6)  -    0x00777000  (9+9)
+		 * 2    ss0 disabled (2x6)         -    0x00777000  (3+9)
+		 * 3    ss1 disabled (2x6)         -    0x00770000  (6+6)
+		 * 4    ss2 disabled (2x6)         -    0x00007000  (9+3)
+		 */
+		u32 eu_pool_config = 0x00777000;
 
 		OUT_BATCH(d, i, GEN9_MEDIA_POOL_STATE);
 		OUT_BATCH(d, i, GEN9_MEDIA_POOL_ENABLE);
-		OUT_BATCH(d, i, pool_config);
+		OUT_BATCH(d, i, eu_pool_config);
 		OUT_BATCH(d, i, 0);
 		OUT_BATCH(d, i, 0);
 		OUT_BATCH(d, i, 0);
diff --git a/include/uapi/drm/i915_drm.h b/include/uapi/drm/i915_drm.h
index 41c6ed9..5041e51 100644
--- a/include/uapi/drm/i915_drm.h
+++ b/include/uapi/drm/i915_drm.h
@@ -371,6 +371,7 @@ typedef struct drm_i915_irq_wait {
 /* Private (not upstreamed) parameters start from 0x800   */
 /* This helps to avoid conflicts with new upstream values */
 #define I915_PARAM_HAS_POOLED_EU         0x800
+#define I915_PARAM_MIN_EU_IN_POOL        0x801
 
 typedef struct drm_i915_getparam {
 	__s32 param;
-- 
1.7.1


From c444aa4aed5a8ff8e1d4e75c11c796e9843b4c10 Mon Sep 17 00:00:00 2001
From: Jacek Danecki <jacek.danecki@intel.com>
Date: Fri, 2 Dec 2016 14:01:35 +0100
Subject: [PATCH 17/17] Implement WaDisablePooledEuLoadBalancingFix

Tracked-On:
For: GMINL-10254
Change-Id: I0c7ec878db3e7b2d80c2fa617ac10db94bab2bd4
Signed-off-by: Robert Beckett <robert.beckett@intel.com>
Signed-off-by: Jeff McGee <jeff.mcgee@intel.com>
Signed-off-by: Matt Roper <matthew.d.roper@intel.com>
---
 drivers/gpu/drm/i915/i915_reg.h         |    1 +
 drivers/gpu/drm/i915/intel_ringbuffer.c |    4 ++++
 2 files changed, 5 insertions(+), 0 deletions(-)

diff --git a/drivers/gpu/drm/i915/i915_reg.h b/drivers/gpu/drm/i915/i915_reg.h
index b899127..e627421 100644
--- a/drivers/gpu/drm/i915/i915_reg.h
+++ b/drivers/gpu/drm/i915/i915_reg.h
@@ -6085,6 +6085,7 @@ enum skl_disp_power_wells {
 #define   GEN9_FFSC_PERCTX_PREEMPT_CTRL	(1<<14)
 
 #define FF_SLICE_CS_CHICKEN2			_MMIO(0x20e4)
+#define  GEN9_POOLED_EU_LOAD_BALANCE_FIX_DISABLE (1<<10)
 #define  GEN9_TSG_BARRIER_ACK_DISABLE		(1<<8)
 
 #define GEN9_CS_DEBUG_MODE1		_MMIO(0x20ec)
diff --git a/drivers/gpu/drm/i915/intel_ringbuffer.c b/drivers/gpu/drm/i915/intel_ringbuffer.c
index 33e9de1..5e28579 100644
--- a/drivers/gpu/drm/i915/intel_ringbuffer.c
+++ b/drivers/gpu/drm/i915/intel_ringbuffer.c
@@ -1190,6 +1190,10 @@ static int bxt_init_workarounds(struct intel_engine_cs *engine)
 		WA_SET_BIT_MASKED(COMMON_SLICE_CHICKEN2,
 				  GEN8_SBE_DISABLE_REPLAY_BUF_OPTIMIZATION);
 
+	/* WaDisablePooledEuLoadBalancingFix:bxt */
+	WA_SET_BIT_MASKED(FF_SLICE_CS_CHICKEN2,
+			  GEN9_POOLED_EU_LOAD_BALANCE_FIX_DISABLE);
+
 	return 0;
 }
 
-- 
1.7.1

